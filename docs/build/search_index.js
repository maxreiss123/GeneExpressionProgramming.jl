var documenterSearchIndex = {"docs":
[{"location":"getting-started.html#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"This guide provides a comprehensive introduction to using GeneExpressionProgramming.jl for symbolic regression tasks. Whether you're new to genetic programming or an experienced practitioner, this guide will help you understand the core concepts and get you running your first symbolic regression experiments quickly.","category":"page"},{"location":"getting-started.html#What-is-Gene-Expression-Programming?","page":"Getting Started","title":"What is Gene Expression Programming?","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Gene Expression Programming (GEP) is an evolutionary algorithm that evolves mathematical expressions to solve symbolic regression problems. Unlike traditional genetic programming that uses tree structures, GEP uses linear chromosomes that are translated into expression trees, providing several advantages including faster processing and more efficient genetic operations.","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"The key innovation of GEP lies in its separation of genotype (linear chromosome) and phenotype (expression tree), allowing for more flexible and efficient evolution of mathematical expressions. This package implements GEP with modern Julia optimizations and additional features like multi-objective optimization and physical dimensionality constraints.","category":"page"},{"location":"getting-started.html#Basic-Workflow","page":"Getting Started","title":"Basic Workflow","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"The typical workflow for using GeneExpressionProgramming.jl follows these steps:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Data Preparation: Organize your input features and target values\nRegressor Creation: Initialize a GEP regressor with appropriate parameters\nModel Fitting: Train the regressor using evolutionary algorithms\nPrediction: Use the trained model to make predictions on new data\nAnalysis: Examine the evolved expressions and their performance","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Let's walk through each step with practical examples.","category":"page"},{"location":"getting-started.html#Your-First-Symbolic-Regression","page":"Getting Started","title":"Your First Symbolic Regression","text":"","category":"section"},{"location":"getting-started.html#Step-1:-Import-Required-Packages","page":"Getting Started","title":"Step 1: Import Required Packages","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"using GeneExpressionProgramming\nusing Random\nusing Plots  # Optional, for visualization","category":"page"},{"location":"getting-started.html#Step-2:-Generate-Sample-Data","page":"Getting Started","title":"Step 2: Generate Sample Data","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"For this example, we'll create a synthetic dataset with a known mathematical relationship:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"# Set random seed for reproducibility\nRandom.seed!(42)\n\n# Define the number of features\nnumber_features = 2\n\n# Generate random input data\nn_samples = 100\nx_data = randn(Float64, n_samples, number_features)\n\n# Define the true function: f(x1, x2) = x1² + x1*x2 - 2*x1*x2\ny_data = @. x_data[:,1]^2 + x_data[:,1] * x_data[:,2] - 2 * x_data[:,1] * x_data[:,2]\n\n# Add some noise to make it more realistic\ny_data += 0.1 * randn(n_samples)","category":"page"},{"location":"getting-started.html#Step-3:-Create-and-Configure-the-Regressor","page":"Getting Started","title":"Step 3: Create and Configure the Regressor","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"# Create a GEP regressor\nregressor = GepRegressor(number_features)\n\n# Define evolution parameters\nepochs = 1000          # Number of generations\npopulation_size = 1000 # Size of the population","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"The GepRegressor constructor accepts various parameters to customize the evolutionary process. For now, we're using the default settings, which work well for most problems.","category":"page"},{"location":"getting-started.html#Step-4:-Train-the-Model","page":"Getting Started","title":"Step 4: Train the Model","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"# Fit the regressor to the data\nfit!(regressor, epochs, population_size, x_data', y_data; loss_fun=\"mse\")","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Note that we transpose x_data because GeneExpressionProgramming.jl expects features as rows and samples as columns, following Julia's column-major convention.","category":"page"},{"location":"getting-started.html#Step-5:-Make-Predictions-and-Analyze-Results","page":"Getting Started","title":"Step 5: Make Predictions and Analyze Results","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"# Make predictions on the training data\npredictions = regressor(x_data')\n\n# Display the best evolved expression\nprintln(\"Best expression: \", regressor.best_models_[1].compiled_function)\nprintln(\"Fitness (MSE): \", regressor.best_models_[1].fitness)\n\n# Calculate R² score\nfunction r_squared(y_true, y_pred)\n    ss_res = sum((y_true .- y_pred).^2)\n    ss_tot = sum((y_true .- mean(y_true)).^2)\n    return 1 - ss_res / ss_tot\nend\n\nr2 = r_squared(y_data, predictions)\nprintln(\"R² Score: \", r2)","category":"page"},{"location":"getting-started.html#Step-6:-Visualize-Results-(Optional)","page":"Getting Started","title":"Step 6: Visualize Results (Optional)","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"# Create a scatter plot comparing actual vs predicted values\nscatter(y_data, predictions, \n        xlabel=\"Actual Values\", \n        ylabel=\"Predicted Values\",\n        title=\"Actual vs Predicted Values\",\n        legend=false,\n        alpha=0.6)\n\n# Add perfect prediction line\nplot!([minimum(y_data), maximum(y_data)], \n      [minimum(y_data), maximum(y_data)], \n      color=:red, \n      linestyle=:dash,\n      label=\"Perfect Prediction\")","category":"page"},{"location":"getting-started.html#Understanding-the-Results","page":"Getting Started","title":"Understanding the Results","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"When you run the above code, GeneExpressionProgramming.jl will evolve mathematical expressions over the specified number of generations. The algorithm will try to find expressions that minimize the mean squared error between predictions and actual values.","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"The output will show you:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Best Expression: The mathematical formula that best fits your data\nFitness: The loss value (lower is better for MSE)\nR² Score: Coefficient of determination (closer to 1 is better)","category":"page"},{"location":"getting-started.html#Interpreting-Evolved-Expressions","page":"Getting Started","title":"Interpreting Evolved Expressions","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"The evolved expressions use standard mathematical notation:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"x1, x2, etc. represent your input features\nCommon operators include +, -, *, /, ^\nFunctions like sin, cos, exp, log may appear depending on the function set","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"For example, an evolved expression might look like:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"x1 * x1 + x1 * x2 - 2.0 * x1 * x2","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"This closely matches our original function, demonstrating the algorithm's ability to discover the underlying mathematical relationship.","category":"page"},{"location":"getting-started.html#Working-with-Real-Data","page":"Getting Started","title":"Working with Real Data","text":"","category":"section"},{"location":"getting-started.html#Loading-Data-from-Files","page":"Getting Started","title":"Loading Data from Files","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"For real-world applications, you'll typically load data from files:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"using CSV, DataFrames\n\n# Load data from CSV\ndf = CSV.read(\"your_data.csv\", DataFrame)\n\n# Extract features and target\nfeature_columns = [:feature1, :feature2, :feature3]  # Adjust column names\ntarget_column = :target\n\nx_data = Matrix(df[:, feature_columns])\ny_data = df[:, target_column]\n\n# Get number of features\nnumber_features = length(feature_columns)","category":"page"},{"location":"getting-started.html#Data-Preprocessing","page":"Getting Started","title":"Data Preprocessing","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Before training, consider preprocessing your data:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"# Normalize features (optional but often helpful)\nusing Statistics\n\nfunction normalize_features(X)\n    X_norm = copy(X)\n    for i in 1:size(X, 2)\n        col_mean = mean(X[:, i])\n        col_std = std(X[:, i])\n        if col_std > 0\n            X_norm[:, i] = (X[:, i] .- col_mean) ./ col_std\n        end\n    end\n    return X_norm\nend\n\nx_data_normalized = normalize_features(x_data)","category":"page"},{"location":"getting-started.html#Train-Test-Split","page":"Getting Started","title":"Train-Test Split","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"For proper evaluation, split your data into training and testing sets:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"function train_test_split(X, y; test_ratio=0.2, random_state=42)\n    Random.seed!(random_state)\n    n_samples = size(X, 1)\n    n_test = round(Int, n_samples * test_ratio)\n    \n    indices = randperm(n_samples)\n    test_indices = indices[1:n_test]\n    train_indices = indices[n_test+1:end]\n    \n    return X[train_indices, :], X[test_indices, :], y[train_indices], y[test_indices]\nend\n\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data)","category":"page"},{"location":"getting-started.html#Customizing-the-Regressor","page":"Getting Started","title":"Customizing the Regressor","text":"","category":"section"},{"location":"getting-started.html#Basic-Parameters","page":"Getting Started","title":"Basic Parameters","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"The GepRegressor constructor accepts several parameters to customize the evolutionary process:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"regressor = GepRegressor(\n    number_features;\n    gene_count = 2,             # Number of genes per chromosome\n    head_len = 7,                # Head length of genes\n    rnd_count = 2, \t\t# assign the number of utilized random numbers\n    tail_weigths = [0.6,0.2,0.2],  # assign utlized prob. for the sampled symbols =>  [features, constants, random numbers]\n    gene_connections=[:+, :-, :*, :/], #defines how the genes can be connnected\n    entered_terminal_nums = [Symbol(0.0), Symbol(0.5)] # define some constant values\n    \n)","category":"page"},{"location":"getting-started.html#Function-Set-Customization","page":"Getting Started","title":"Function Set Customization","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"You can customize the function set used in evolution:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"# Define custom function set\ncustom_functions = [\n    :+, :-, :*, :/,              # Basic arithmetic\n    :sin, :cos, :exp, :log,      # Transcendental functions\n    :sqrt, :abs                  # Other functions\n]\n\nregressor = GepRegressor(number_features; entered_non_terminals=custom_functions)","category":"page"},{"location":"getting-started.html#Loss-Function-Options","page":"Getting Started","title":"Loss Function Options","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"GeneExpressionProgramming.jl supports various loss functions:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"# Mean Squared Error (default)\nfit!(regressor, epochs, population_size, x_train', y_train; loss_fun=\"mse\")\n\n# Mean Absolute Error\nfit!(regressor, epochs, population_size, x_train', y_train; loss_fun=\"mae\")\n\n# Root Mean Squared Error\nfit!(regressor, epochs, population_size, x_train', y_train; loss_fun=\"rmse\")","category":"page"},{"location":"getting-started.html#Monitoring-Training-Progress","page":"Getting Started","title":"Monitoring Training Progress","text":"","category":"section"},{"location":"getting-started.html#Fitness-History","page":"Getting Started","title":"Fitness History","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"You can monitor the training progress by accessing the fitness history:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"# After training\nfitness_history = [elem[1] for elem in regressor.fitness_history_.train_loss] # save as tuple within the history\n\n# Plot fitness over generations\nplot(1:length(fitness_history), fitness_history,\n     xlabel=\"Generation\",\n     ylabel=\"Fitness (MSE)\",\n     title=\"Training Progress\",\n     legend=false)","category":"page"},{"location":"getting-started.html#Best-Practices","page":"Getting Started","title":"Best Practices","text":"","category":"section"},{"location":"getting-started.html#1.-Start-Simple","page":"Getting Started","title":"1. Start Simple","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Begin with basic parameters and gradually increase complexity:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Start with smaller population sizes (100-500) for quick experiments\nUse fewer generations initially to test your setup\nGradually increase complexity as needed","category":"page"},{"location":"getting-started.html#2.-Data-Quality","page":"Getting Started","title":"2. Data Quality","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Ensure your data is clean and well-prepared:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Remove or handle missing values appropriately\nConsider feature scaling for better convergence\nEnsure sufficient data for the complexity of the target function","category":"page"},{"location":"getting-started.html#3.-Parameter-Tuning","page":"Getting Started","title":"3. Parameter Tuning","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Experiment with different parameter combinations:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Population Size: Larger populations explore more solutions but require more computation\nGenerations: More generations allow for better solutions but take longer\nGene Count: More genes can represent more complex functions\nHead Length: Longer heads allow for more complex expressions","category":"page"},{"location":"getting-started.html#4.-Validation","page":"Getting Started","title":"4. Validation","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Always validate your results on unseen data:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Use train-test splits or cross-validation\nCheck for overfitting by comparing training and test performance\nConsider the interpretability of evolved expressions","category":"page"},{"location":"getting-started.html#Common-Issues-and-Solutions","page":"Getting Started","title":"Common Issues and Solutions","text":"","category":"section"},{"location":"getting-started.html#Issue-1:-Poor-Convergence","page":"Getting Started","title":"Issue 1: Poor Convergence","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"If the algorithm doesn't find good solutions:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Increase population size or number of generations\nAdjust mutation and crossover rates\nTry different selection methods\nCheck data quality and preprocessing","category":"page"},{"location":"getting-started.html#Issue-2:-Overly-Complex-Expressions","page":"Getting Started","title":"Issue 2: Overly Complex Expressions","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"If evolved expressions are too complex:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Reduce head length or gene count\nUse multi-objective optimization to balance accuracy and complexity\nImplement expression simplification post-processing","category":"page"},{"location":"getting-started.html#Issue-3:-Slow-Performance","page":"Getting Started","title":"Issue 3: Slow Performance","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"If training is too slow:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Reduce population size for initial experiments\nUse fewer generations with early stopping\nConsider parallel processing options\nProfile your code to identify bottlenecks","category":"page"},{"location":"getting-started.html#Next-Steps","page":"Getting Started","title":"Next Steps","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Now that you understand the basics, explore more advanced features:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Multi-Objective Optimization: Balance accuracy and complexity\nPhysical Dimensionality: Ensure dimensional consistency\nTensor Regression: Work with vector and matrix data","category":"page"},{"location":"getting-started.html#Summary","page":"Getting Started","title":"Summary","text":"","category":"section"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"In this guide, you learned:","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"The basic workflow for symbolic regression with GeneExpressionProgramming.jl\nHow to prepare data and configure the regressor\nHow to train models and interpret results\nBest practices for successful symbolic regression\nCommon issues and their solutions","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"The power of GeneExpressionProgramming.jl lies in its ability to automatically discover mathematical relationships in your data while providing interpretable results. As you become more familiar with the package, you can explore its advanced features to tackle more complex problems and achieve better results.","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"","category":"page"},{"location":"getting-started.html","page":"Getting Started","title":"Getting Started","text":"Continue to Core Concepts to deepen your understanding of the underlying algorithms and theory.","category":"page"},{"location":"core-concepts.html#Core-Concepts","page":"Core Concepts","title":"Core Concepts","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"This chapter provides a comprehensive overview of the theoretical foundations and key concepts underlying GeneExpressionProgramming.jl. Understanding these concepts will help you make informed decisions about parameter settings, interpret results effectively, and apply the package to complex real-world problems.","category":"page"},{"location":"core-concepts.html#Gene-Expression-Programming-Fundamentals","page":"Core Concepts","title":"Gene Expression Programming Fundamentals","text":"","category":"section"},{"location":"core-concepts.html#Historical-Context-and-Evolution","page":"Core Concepts","title":"Historical Context and Evolution","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Gene Expression Programming (GEP) was introduced by Cândida Ferreira in 2001 as an evolutionary algorithm that combines the advantages of both genetic algorithms and genetic programming [1]. GEP addresses several limitations of traditional genetic programming, particularly the issues of closure and structural complexity that can hinder the evolution of mathematical expressions.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"The key innovation of GEP lies in its unique representation scheme that separates the genotype (linear chromosome) from the phenotype (expression tree). This separation allows for more efficient genetic operations while maintaining the expressiveness needed for symbolic regression tasks. Unlike genetic programming, which directly manipulates tree structures, GEP operates on fixed-length linear chromosomes that are then translated into expression trees of varying sizes and shapes.","category":"page"},{"location":"core-concepts.html#Genotype-Phenotype-Mapping","page":"Core Concepts","title":"Genotype-Phenotype Mapping","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"The fundamental concept in GEP is the translation from linear chromosomes (genotype) to expression trees (phenotype). This process involves several key components:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Chromosomes: In GEP, a chromosome consists of one or more genes, each representing a sub-expression. Each gene has a fixed length and is divided into a head and a tail. The head can contain both functions and terminals, while the tail contains only terminals.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Head and Tail Structure: The head length (h) determines the maximum complexity of the expression, while the tail length is calculated as t = h × (n-1) + 1, where n is the maximum arity of the functions in the function set. This ensures that any expression can be properly formed regardless of the arrangement of functions and terminals.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Translation Process: The linear chromosome is read from left to right, and the expression tree is constructed using a breadth-first approach. Functions require operands according to their arity, and the translation continues until all function nodes have been satisfied.","category":"page"},{"location":"core-concepts.html#Expression-Trees-and-Evaluation","page":"Core Concepts","title":"Expression Trees and Evaluation","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Once translated from the linear chromosome, the expression tree represents the mathematical formula that will be evaluated. The tree structure follows standard mathematical conventions:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Internal nodes represent functions (operators)\nLeaf nodes represent terminals (variables or constants)\nEvaluation proceeds from the leaves to the root using standard mathematical precedence","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"The expression tree evaluation in GeneExpressionProgramming.jl is optimized using DynamicExpressions.jl, which provides fast compilation and evaluation of symbolic expressions. This optimization is crucial for the performance of the evolutionary algorithm, as fitness evaluation typically dominates the computational cost.","category":"page"},{"location":"core-concepts.html#Evolutionary-Operators","page":"Core Concepts","title":"Evolutionary Operators","text":"","category":"section"},{"location":"core-concepts.html#Selection-Mechanisms","page":"Core Concepts","title":"Selection Mechanisms","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"GeneExpressionProgramming.jl implements several selection mechanisms to choose parents for reproduction:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Tournament Selection: The default selection method where k individuals are randomly chosen from the population, and the best among them is selected as a parent. Tournament size affects selection pressure - larger tournaments increase pressure toward fitter individuals.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"NSGA-II Selection: For multi-objective optimization, the package implements the Non-dominated Sorting Genetic Algorithm II (NSGA-II), which maintains diversity while progressing toward the Pareto front. This is particularly useful when balancing accuracy against expression complexity.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Roulette Wheel Selection: Probabilistic selection based on fitness proportions, though this is less commonly used due to potential issues with fitness scaling.","category":"page"},{"location":"core-concepts.html#Genetic-Operators","page":"Core Concepts","title":"Genetic Operators","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Mutation: GEP uses several mutation operators that work directly on the linear chromosome:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Point Mutation: Randomly changes individual symbols in the chromosome\nInversion: Reverses a sequence of symbols within a gene\nIS Transposition: Inserts a sequence from one location to another\nRIS Transposition: Similar to IS but with root insertion\nGene Transposition: Moves entire genes within the chromosome","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Crossover: Recombination operators that combine genetic material from two parents:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"One-Point Crossover: Exchanges genetic material at a single crossover point\nTwo-Point Crossover: Uses two crossover points to exchange a middle segment\nGene Crossover: Exchanges entire genes between parents\nUniform Crossover: Exchanges individual symbols with a certain probability","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Recombination: Higher-level operators that combine sub-expressions:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"One-Point Recombination: Exchanges sub-trees at randomly chosen points\nTwo-Point Recombination: Exchanges sub-trees between two points","category":"page"},{"location":"core-concepts.html#Population-Dynamics","page":"Core Concepts","title":"Population Dynamics","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"The evolutionary process in GEP follows a generational model where the entire population is replaced in each generation, except for elite individuals that are preserved through elitism. The population dynamics are controlled by several parameters:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Population Size: Determines the number of individuals in each generation. Larger populations provide better exploration but require more computational resources.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Generation Gap: The proportion of the population replaced in each generation. A generation gap of 1.0 means the entire population (except elites) is replaced.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Elitism: The number of best individuals preserved from one generation to the next, ensuring that good solutions are not lost during evolution.","category":"page"},{"location":"core-concepts.html#Multi-Objective-Optimization","page":"Core Concepts","title":"Multi-Objective Optimization","text":"","category":"section"},{"location":"core-concepts.html#Pareto-Optimality","page":"Core Concepts","title":"Pareto Optimality","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"In many real-world applications, there are multiple conflicting objectives that need to be optimized simultaneously. For symbolic regression, common objectives include:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Accuracy: Minimizing prediction error (MSE, MAE, etc.)\nComplexity: Minimizing expression size or depth\nInterpretability: Favoring simpler, more understandable expressions","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"GeneExpressionProgramming.jl implements multi-objective optimization using NSGA-II, which maintains a diverse set of solutions along the Pareto front. A solution is Pareto optimal if no other solution exists that is better in all objectives simultaneously.","category":"page"},{"location":"core-concepts.html#Crowding-Distance","page":"Core Concepts","title":"Crowding Distance","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"To maintain diversity in the population, NSGA-II uses crowding distance as a secondary selection criterion. Individuals in less crowded regions of the objective space are preferred, encouraging the algorithm to explore the entire Pareto front rather than converging to a single region.","category":"page"},{"location":"core-concepts.html#Objective-Function-Design","page":"Core Concepts","title":"Objective Function Design","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"When designing multi-objective problems, consider:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Objective Scaling: Ensure objectives are on comparable scales\nObjective Correlation: Highly correlated objectives may not provide additional information\nObjective Conflict: Objectives should represent genuine trade-offs","category":"page"},{"location":"core-concepts.html#Physical-Dimensionality-and-Semantic-Backpropagation","page":"Core Concepts","title":"Physical Dimensionality and Semantic Backpropagation","text":"","category":"section"},{"location":"core-concepts.html#Dimensional-Analysis","page":"Core Concepts","title":"Dimensional Analysis","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"One of the unique features of GeneExpressionProgramming.jl is its support for physical dimensionality constraints through semantic backpropagation [2]. This feature ensures that evolved expressions respect physical units and dimensional homogeneity.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Dimensional Representation: Physical dimensions are represented as vectors where each component corresponds to a fundamental unit (mass, length, time, electric current, temperature, amount of substance, luminous intensity).","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Forward Propagation: During expression evaluation, dimensions are propagated forward through the expression tree according to the rules of dimensional analysis:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Addition/Subtraction: Operands must have the same dimensions\nMultiplication: Dimensions are added\nDivision: Dimensions are subtracted\nPower: Dimensions are multiplied by the exponent","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Backward Propagation: When dimensional inconsistencies are detected, the algorithm can backpropagate corrections to ensure dimensional validity.","category":"page"},{"location":"core-concepts.html#Semantic-Constraints","page":"Core Concepts","title":"Semantic Constraints","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Semantic backpropagation goes beyond simple dimensional analysis to enforce more complex semantic constraints:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Type Consistency: Ensuring that operations are semantically meaningful (e.g., not taking the logarithm of a dimensioned quantity)","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Domain Constraints: Restricting function domains (e.g., ensuring arguments to sqrt are non-negative)","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Physical Plausibility: Enforcing constraints based on physical laws and principles","category":"page"},{"location":"core-concepts.html#Tensor-Operations-and-Advanced-Data-Types","page":"Core Concepts","title":"Tensor Operations and Advanced Data Types","text":"","category":"section"},{"location":"core-concepts.html#Tensor-Regression","page":"Core Concepts","title":"Tensor Regression","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"For problems involving vector or matrix data, GeneExpressionProgramming.jl provides tensor regression capabilities through the GepTensorRegressor. This functionality is built on top of Flux.jl and supports:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Vector Operations: Element-wise operations, dot products, cross products, and norms","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Matrix Operations: Matrix multiplication, transposition, determinants, and eigenvalue operations","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Tensor Contractions: General tensor operations for higher-dimensional data","category":"page"},{"location":"core-concepts.html#Performance-Considerations","page":"Core Concepts","title":"Performance Considerations","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Tensor operations are computationally more expensive than scalar operations, so several optimizations are implemented:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Lazy Evaluation: Operations are only computed when needed","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Batch Processing: Multiple evaluations are batched together for efficiency","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"GPU Acceleration:  not yet implemented","category":"page"},{"location":"core-concepts.html#Loss-Functions-and-Fitness-Evaluation","page":"Core Concepts","title":"Loss Functions and Fitness Evaluation","text":"","category":"section"},{"location":"core-concepts.html#Standard-Loss-Functions","page":"Core Concepts","title":"Standard Loss Functions","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"GeneExpressionProgramming.jl provides various regression losses:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Mean Squared Error (MSE): Standard for continuous regression\nMean Absolute Error (MAE): Robust to outliers\nRoot Mean Squared Error (RMSE): Scale-dependent alternative to MSE\nHuber Loss: Combines MSE and MAE properties","category":"page"},{"location":"core-concepts.html#Custom-Loss-Functions","page":"Core Concepts","title":"Custom Loss Functions","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"For specialized applications, you can define custom loss functions:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"function custom_loss(y_true, y_pred)\n    # Your custom loss calculation\n    return loss_value\nend","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Moreover there is a more general wrapper function, where the user can define the evalulation strategy complete independent.","category":"page"},{"location":"core-concepts.html#Fitness-Evaluation-Strategies","page":"Core Concepts","title":"Fitness Evaluation Strategies","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Single-Objective: Direct optimization of a single loss function","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Multi-Objective: Simultaneous optimization of multiple objectives using Pareto dominance","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Lexicographic: Hierarchical optimization where objectives are prioritized","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Weighted Sum: Linear combination of multiple objectives with user-defined weights","category":"page"},{"location":"core-concepts.html#Algorithm-Parameters-and-Tuning","page":"Core Concepts","title":"Algorithm Parameters and Tuning","text":"","category":"section"},{"location":"core-concepts.html#Population-Parameters","page":"Core Concepts","title":"Population Parameters","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Population Size: Affects exploration vs. exploitation trade-off","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Small populations (50-200): Fast convergence, limited exploration\nMedium populations (200-1000): Balanced performance\nLarge populations (1000+): Extensive exploration, slower convergence","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Number of Generations: Determines total computational budget","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Early stopping can be used to prevent overfitting\nMonitor convergence to determine appropriate stopping criteria","category":"page"},{"location":"core-concepts.html#Genetic-Parameters","page":"Core Concepts","title":"Genetic Parameters","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Mutation Rate: Controls exploration intensity","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Low rates (0.01-0.05): Conservative exploration\nMedium rates (0.05-0.15): Balanced exploration\nHigh rates (0.15+): Aggressive exploration, risk of disruption","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Crossover Rate: Controls exploitation of existing solutions","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Typically set between 0.6-0.9 for good performance\nHigher rates emphasize recombination over mutation","category":"page"},{"location":"core-concepts.html#Expression-Parameters","page":"Core Concepts","title":"Expression Parameters","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Head Length: Determines maximum expression complexity","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Shorter heads: Simpler expressions, faster evaluation\nLonger heads: More complex expressions, higher computational cost","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Gene Count: Number of sub-expressions per chromosome","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Single genes: Simple expressions\nMultiple genes: Complex, modular expressions (e.g different terms within the problem considered)","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Function Set: Available operations for expression construction","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Basic arithmetic: +, -, *, /\nTranscendental: sin, cos, exp, log\nSpecialized: domain-specific functions","category":"page"},{"location":"core-concepts.html#Performance-Optimization","page":"Core Concepts","title":"Performance Optimization","text":"","category":"section"},{"location":"core-concepts.html#Computational-Efficiency","page":"Core Concepts","title":"Computational Efficiency","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Expression Compilation: DynamicExpressions.jl compiles expressions for fast evaluation","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Parallel Evaluation: Population evaluation can be parallelized across multiple cores","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Memory Management: Efficient memory usage for large populations and long chromosomes","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Caching: Intermediate results are cached to avoid redundant computations (cache blow up might remain an issue)","category":"page"},{"location":"core-concepts.html#Scalability-Considerations","page":"Core Concepts","title":"Scalability Considerations","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Problem Size: Algorithm complexity scales with:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Population size\nExpression complexity\nNumber of generations\nDataset size","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Memory Requirements: Dominated by population storage and expression evaluation","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Parallel Processing: Multiple levels of parallelization available:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Population-level: Evaluate individuals in parallel\nExpression-level: Parallel evaluation of sub-expressions\nData-level: Parallel evaluation across data samples","category":"page"},{"location":"core-concepts.html#Theoretical-Foundations","page":"Core Concepts","title":"Theoretical Foundations","text":"","category":"section"},{"location":"core-concepts.html#Convergence-Properties","page":"Core Concepts","title":"Convergence Properties","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"GEP belongs to the class of evolutionary algorithms with the following theoretical properties:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Schema Theorem: Building blocks (schemas) with above-average fitness tend to receive exponentially increasing numbers of trials in subsequent generations [3]","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"No Free Lunch Theorem: No single algorithm performs best on all possible problems, emphasizing the importance of problem-specific tuning [4]","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Exploration vs. Exploitation: The algorithm must balance exploring new regions of the search space with exploiting known good solutions","category":"page"},{"location":"core-concepts.html#Search-Space-Characteristics","page":"Core Concepts","title":"Search Space Characteristics","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Representation Space: The space of all possible linear chromosomes","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Phenotype Space: The space of all possible expression trees","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Fitness Landscape: The mapping from expressions to fitness values, which can be:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Smooth: Gradual fitness changes with small modifications\nRugged: Many local optima and fitness discontinuities\nNeutral: Large regions with similar fitness values","category":"page"},{"location":"core-concepts.html#Complexity-Analysis","page":"Core Concepts","title":"Complexity Analysis","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Time Complexity: O(G × P × L × N) where:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"G: Number of generations\nP: Population size (the one evaluated wihtin each generation)\nL: Average expression complexity (traversing the expression tree)\nN: Dataset size","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Space Complexity: O(P × L) for population storage plus O(N) for dataset storage (assuming the )","category":"page"},{"location":"core-concepts.html#Advanced-Topics","page":"Core Concepts","title":"Advanced Topics","text":"","category":"section"},{"location":"core-concepts.html#Hybrid-Approaches","page":"Core Concepts","title":"Hybrid Approaches","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"GeneExpressionProgramming.jl can be combined with other optimization techniques:","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Local Search: Gradient-based optimization of numerical constants","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Ensemble Methods: Combining multiple evolved expressions","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Transfer Learning: Using knowledge from related problems","category":"page"},{"location":"core-concepts.html#Domain-Specific-Extensions","page":"Core Concepts","title":"Domain-Specific Extensions","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Time Series: Specialized operators for temporal data","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Image Processing: Operators for spatial data and convolution","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Signal Processing: Frequency domain operations and filtering","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"Scientific Computing: Domain-specific function sets for physics, chemistry, biology","category":"page"},{"location":"core-concepts.html#References","page":"Core Concepts","title":"References","text":"","category":"section"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"[1] Ferreira, C. (2001). Gene Expression Programming: a New Adaptive Algorithm for Solving Problems. Complex Systems, 13.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"[2] Reissmann, M., Fang, Y., Ooi, A. S. H., & Sandberg, R. D. (2025). Constraining genetic symbolic regression via semantic backpropagation. Genetic Programming and Evolvable Machines, 26(1), 12.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"[3] Holland, J. H. (1975). Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence. University of Michigan Press.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"[4] Wolpert, D. H., & Macready, W. G. (1997). No Free Lunch Theorems for Optimization. IEEE Transactions on Evolutionary Computation, 1(1), 67–82.","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"","category":"page"},{"location":"core-concepts.html","page":"Core Concepts","title":"Core Concepts","text":"This chapter provides the theoretical foundation for understanding GeneExpressionProgramming.jl. For practical applications, continue to the API Reference.","category":"page"},{"location":"api-reference.html#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"This comprehensive API reference provides detailed documentation for all public functions, types, and modules in GeneExpressionProgramming.jl. The API is organized by functionality to help you quickly find the components you need.","category":"page"},{"location":"api-reference.html#Core-Types","page":"API Reference","title":"Core Types","text":"","category":"section"},{"location":"api-reference.html#GepRegressor","page":"API Reference","title":"GepRegressor","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"The main regressor for scalar symbolic regression tasks.","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"GepRegressor(number_features::Int; kwargs...)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Parameters:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"number_features::Int: Number of input features\ngene_count::Int = 2: Number of genes per chromosome\nhead_len::Int = 7: Head length of each gene\nrnd_count::Int = 5: Amount of rondom numbers beeing considered  \nentered_non_terminals::Vector{Symbol} = [:+, :-, :*, :/]: Available functions\ngene_connections::Vector{Symbol} = [:+, :-, :*, :/]: Functions for connecting the genes\nnumber_of_objectives::Int = 1: Number of objectives (1 for single-objective)\nconsidered_dimensions::Dict{Symbol,Vector{Float16}} = Dict(): Physical dimensions\nmax_permutations_lib::Int = 1000: Maximum permutations for dimensional analysis\nrounds::Int = 5: Tree depth for dimensional checking","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Fields:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"best_models_::Vector: Best evolved models\nfitness_history_: Training history (if available)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Example:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"regressor = GepRegressor(3; \n                        gene_count=3,\n                        head_len=5,\n                        entered_non_terminals=[:+, :-, :*, :/, :sin, :cos])","category":"page"},{"location":"api-reference.html#GepTensorRegressor","page":"API Reference","title":"GepTensorRegressor","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Specialized regressor for tensor (vector/matrix) symbolic regression.","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"GepTensorRegressor(number_features::Int, gene_count::Int, head_len::Int; kwargs...)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Parameters:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"number_features::Int: Number of input features\ngene_count::Int: Number of genes per chromosome\nhead_len::Int: Head length of each gene\nfeature_names::Vector{String} = []: Names for features (for interpretability)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Example:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"regressor = GepTensorRegressor(5, 2, 3; \n                              feature_names=[\"x1\", \"x2\", \"U1\", \"U2\", \"U3\"])","category":"page"},{"location":"api-reference.html#Core-Functions","page":"API Reference","title":"Core Functions","text":"","category":"section"},{"location":"api-reference.html#fit!","page":"API Reference","title":"fit!","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Train the GEP regressor on data.","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"fit!(regressor, epochs::Int, population_size::Int, x_data, y_data; kwargs...)\nfit!(regressor, epochs::Int, population_size::Int, loss_function)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Parameters:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"regressor: GepRegressor or GepTensorRegressor instance\nepochs::Int: Number of generations to evolve\npopulation_size::Int: Population size for evolution\nx_data: Input features (features as rows, samples as columns)\ny_data: Target values\nloss_function: Custom loss function (for tensor regression or multi objective)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Keyword Arguments:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"x_test = nothing: Test features for validation\ny_test = nothing: Test targets for validation\nloss_fun::Function = \"function\": Loss function self defined by the user to guide the search\ntarget_dimension = nothing: Target physical dimension","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Examples:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"# Basic regression\nfit!(regressor, 1000, 1000, x_train', y_train; loss_fun=\"mse\")\n\n# With validation data\nfit!(regressor, 1000, 1000, x_train', y_train; \n     x_test=x_test', y_test=y_test, loss_fun=\"rmse\")\n\n# With physical dimensions\nfit!(regressor, 1000, 1000, x_train', y_train; \n     target_dimension=target_dim)\n\n# Tensor regression with custom loss\nfit!(regressor, 100, 500, custom_loss_function)","category":"page"},{"location":"api-reference.html#Prediction","page":"API Reference","title":"Prediction","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Make predictions using trained regressor.","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"(regressor::GepRegressor)(x_data)\n(regressor::GepTensorRegressor)(input_data)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Parameters:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"x_data: Input features (features as rows, samples as columns)\ninput_data: Input data tuple for tensor regression","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Returns:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Predictions as vector (scalar regression) or vector of tensors (tensor regression)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Examples:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"# Scalar predictions\npredictions = regressor(x_test')\n\n# Tensor predictions\ntensor_predictions = tensor_regressor(input_tuple)","category":"page"},{"location":"api-reference.html#Utility-Functions","page":"API Reference","title":"Utility Functions","text":"","category":"section"},{"location":"api-reference.html#Data-Utilities","page":"API Reference","title":"Data Utilities","text":"","category":"section"},{"location":"api-reference.html#train*test*split","page":"API Reference","title":"traintestsplit","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"train_test_split(X, y; test_ratio=0.2, random_state=42)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Split data into training and testing sets.","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Parameters:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"X: Feature matrix\ny: Target vector\ntest_ratio::Float64 = 0.2: Proportion of data for testing\nrandom_state::Int = 42: Random seed","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Returns:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"(X_train, X_test, y_train, y_test): Split data","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Example:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"X_train, X_test, y_train, y_test = train_test_split(X, y; test_ratio=0.3)","category":"page"},{"location":"api-reference.html#Expression-Utilities","page":"API Reference","title":"Expression Utilities","text":"","category":"section"},{"location":"api-reference.html#print*karva*strings","page":"API Reference","title":"printkarvastrings","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"print_karva_strings(solution)","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Print the Karva notation representation of an evolved solution.","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Parameters:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"solution: Evolved solution from best_models_","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Example:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"best_solution = regressor.best_models_[1]\nprint_karva_strings(best_solution)","category":"page"},{"location":"api-reference.html#Loss-Functions","page":"API Reference","title":"Loss Functions","text":"","category":"section"},{"location":"api-reference.html#Built-in-Loss-Functions","page":"API Reference","title":"Built-in Loss Functions","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"The package provides several built-in loss functions accessible via string names:","category":"page"},{"location":"api-reference.html#\"mse\"-Mean-Squared-Error","page":"API Reference","title":"\"mse\" - Mean Squared Error","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"mse(y_true, y_pred) = mean((y_true .- y_pred).^2)","category":"page"},{"location":"api-reference.html#\"mae\"-Mean-Absolute-Error","page":"API Reference","title":"\"mae\" - Mean Absolute Error","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"mae(y_true, y_pred) = mean(abs.(y_true .- y_pred))","category":"page"},{"location":"api-reference.html#\"rmse\"-Root-Mean-Squared-Error","page":"API Reference","title":"\"rmse\" - Root Mean Squared Error","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"rmse(y_true, y_pred) = sqrt(mean((y_true .- y_pred).^2))","category":"page"},{"location":"api-reference.html#Custom-Loss-Functions","page":"API Reference","title":"Custom Loss Functions","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"For advanced applications, you can define custom loss functions:","category":"page"},{"location":"api-reference.html#Single-Objective-Custom-Loss","page":"API Reference","title":"Single-Objective Custom Loss","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"function custom_loss(y_true, y_pred)\n    # Your custom loss calculation\n    return loss_value::Float64\nend\n\n# Use with fit!\nfit!(regressor, epochs, population_size, x_data', y_data; loss_fun=custom_loss)","category":"page"},{"location":"api-reference.html#Multi-Objective-Custom-Loss","page":"API Reference","title":"Multi-Objective Custom Loss","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"@inline function multi_objective_loss(elem, validate::Bool)\n    if isnan(mean(elem.fitness)) || validate\n        model = elem.compiled_function\n        \n        try\n            y_pred = model(x_data')\n            \n            # Objective 1: Accuracy\n            mse = mean((y_true .- y_pred).^2)\n            \n            # Objective 2: Complexity\n            complexity = expression_complexity(model) # expression complexity needs to be defined by the user\n            \n            elem.fitness = (mse, complexity)\n        catch\n            elem.fitness = (typemax(Float64), typemax(Float64))\n        end\n    end\nend\n\n# Use with multi-objective regressor\nregressor = GepRegressor(n_features; number_of_objectives=2)\nfit!(regressor, epochs, population_size, multi_objective_loss)","category":"page"},{"location":"api-reference.html#Tensor-Custom-Loss","page":"API Reference","title":"Tensor Custom Loss","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"@inline function tensor_loss(elem, validate::Bool)\n    if isnan(mean(elem.fitness)) || validate\n        model = elem.compiled_function\n        \n        try\n            predictions = model(input_data)\n            \n            # Calculate tensor-specific loss\n            total_error = 0.0\n            for i in 1:length(target_tensors)\n                error = norm(predictions[i] - target_tensors[i])^2\n                total_error += error\n            end\n            \n            elem.fitness = (total_error / length(target_tensors),)\n        catch\n            elem.fitness = (typemax(Float64),)\n        end\n    end\nend","category":"page"},{"location":"api-reference.html#Selection-Methods","page":"API Reference","title":"Selection Methods","text":"","category":"section"},{"location":"api-reference.html#Tournament-Selection","page":"API Reference","title":"Tournament Selection","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Default selection method that chooses the best individul based on the tournament selections.","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Configuration:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"regressor = GepRegressor(n_features)","category":"page"},{"location":"api-reference.html#NSGA-II-Selection","page":"API Reference","title":"NSGA-II Selection","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Multi-objective selection using Non-dominated Sorting Genetic Algorithm II.","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Configuration:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"regressor = GepRegressor(n_features; \n                        number_of_objectives=2)","category":"page"},{"location":"api-reference.html#Genetic-Operators","page":"API Reference","title":"Genetic Operators","text":"","category":"section"},{"location":"api-reference.html#Genetic-Operators-2","page":"API Reference","title":"Genetic Operators","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"The package implements several genetic operators. Here the can be adjusted in advance using the dictinary GENE_COMMON_PROBS, which is available after loading the GeneExpressionProgramming.jl","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Point Mutation: Random symbol replacement\nInversion: Sequence reversal\nIS Transposition: Insertion sequence transposition\nRIS Transposition: Root insertion sequence transposition","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Configuration:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"using GeneExpressionProgramming\n\nGeneExpressionProgramming.RegressionWrapper.GENE_COMMON_PROBS[\"mutation_prob\"] = 1.0 # Probability for a chromosome of facing a mutation\nGeneExpressionProgramming.RegressionWrapper.GENE_COMMON_PROBS[\"mutation_rate\"] = 0.1 # Proportion of the gene beeing changed\n\n\nGeneExpressionProgramming.RegressionWrapper.GENE_COMMON_PROBS[\"inversion_prob\"] = 0.1 # Setting the prob. for the operation to take place \nGeneExpressionProgramming.RegressionWrapper.GENE_COMMON_PROBS[\"reverse_insertion_tail\"] = 0.1 # Setting  IS \nGeneExpressionProgramming.RegressionWrapper.GENE_COMMON_PROBS[\"reverse_insertion\"] = 0.1 # Setting RIS\nGeneExpressionProgramming.RegressionWrapper.GENE_COMMON_PROBS[\"gene_transposition\"] = 0.0  # Setting Transposition\n\n","category":"page"},{"location":"api-reference.html#Crossover-Operators","page":"API Reference","title":"Crossover Operators","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Available crossover operators: Similar to the gene","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"One-Point Crossover: Single crossover point\nTwo-Point Crossover: Two crossover points","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Configuration:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"using GeneExpressionProgramming\n\nGeneExpressionProgramming.RegressionWrapper.GENE_COMMON_PROBS[\"one_point_cross_over_prob\"] = 0.5 # Setting the one-point crossover\nGeneExpressionProgramming.RegressionWrapper.GENE_COMMON_PROBS[\"two_point_cross_over_prob\"] = 0.3 # Setting the two-point crossover","category":"page"},{"location":"api-reference.html#Function-Sets","page":"API Reference","title":"Function Sets","text":"","category":"section"},{"location":"api-reference.html#Basic-Arithmetic","page":"API Reference","title":"Basic Arithmetic","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"basic_functions = [:+, :-, :*, :/]","category":"page"},{"location":"api-reference.html#Extended-Mathematical-Functions","page":"API Reference","title":"Extended Mathematical Functions","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"extended_functions = [:+, :-, :*, :/, :sin, :cos, :tan, :exp, :log, :sqrt, :abs]","category":"page"},{"location":"api-reference.html#Power-Functions","page":"API Reference","title":"Power Functions","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"power_functions = [:^, :sqrt]","category":"page"},{"location":"api-reference.html#Trigonometric-Functions","page":"API Reference","title":"Trigonometric Functions","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"trig_functions = [:sin, :cos, :tan, :asin, :acos, :atan, :sinh, :cosh, :tanh]","category":"page"},{"location":"api-reference.html#Physical-Dimensionality","page":"API Reference","title":"Physical Dimensionality","text":"","category":"section"},{"location":"api-reference.html#Dimension-Representation","page":"API Reference","title":"Dimension Representation","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Physical dimensions are represented as 7-element vectors corresponding to SI base units:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"# [Mass, Length, Time, Temperature, Current, Amount, Luminosity]\nvelocity_dim = Float16[0, 1, -1, 0, 0, 0, 0]    # [L T⁻¹]\nforce_dim = Float16[1, 1, -2, 0, 0, 0, 0]       # [M L T⁻²]\nenergy_dim = Float16[1, 2, -2, 0, 0, 0, 0]      # [M L² T⁻²]","category":"page"},{"location":"api-reference.html#Dimensional-Constraints","page":"API Reference","title":"Dimensional Constraints","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"feature_dims = Dict{Symbol,Vector{Float16}}(\n    :x1 => Float16[1, 0, 0, 0, 0, 0, 0],    # Mass\n    :x2 => Float16[0, 1, 0, 0, 0, 0, 0],    # Length\n    :x3 => Float16[0, 0, 1, 0, 0, 0, 0],    # Time\n)\n\ntarget_dim = Float16[0, 1, -1, 0, 0, 0, 0]  # Velocity\n\nregressor = GepRegressor(3; \n                        considered_dimensions=feature_dims,\n                        max_permutations_lib=10000)\n\nfit!(regressor, epochs, population_size, x_data', y_data; \n     target_dimension=target_dim)","category":"page"},{"location":"api-reference.html#Tensor-Operations-(under-constructions)","page":"API Reference","title":"Tensor Operations (under constructions)","text":"","category":"section"},{"location":"api-reference.html#Supported-Tensor-Types","page":"API Reference","title":"Supported Tensor Types","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"The tensor regression module supports various tensor types through Tensors.jl:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"using Tensors\n\n# Vectors (rank-1 tensors)\nvector_3d = rand(Tensor{1,3})\n\n# Matrices (rank-2 tensors)  \nmatrix_2x2 = rand(Tensor{2,2})\n\n# Higher-order tensors\ntensor_3x3x3 = rand(Tensor{3,3})","category":"page"},{"location":"api-reference.html#Tensor-Operations","page":"API Reference","title":"Tensor Operations","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Available tensor operations include:","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Element-wise operations: +, -, *, /\nTensor products: ⊗ (outer product)\nContractions: ⋅ (dot product), ⊡ (double contraction)\nNorms: norm(), tr() (trace)\nDecompositions: eigen(), svd()","category":"page"},{"location":"api-reference.html#Error-Handling","page":"API Reference","title":"Error Handling","text":"","category":"section"},{"location":"api-reference.html#Common-Error","page":"API Reference","title":"Common Error","text":"","category":"section"},{"location":"api-reference.html#ArgumentError:-collection-must-be-non-empty","page":"API Reference","title":"ArgumentError: collection must be non-empty","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"Thrown when the argument vector for the selection process is empty. This happens, when all the loss returns Inf for all fit values. ","category":"page"},{"location":"api-reference.html#Performance-Tuning","page":"API Reference","title":"Performance Tuning","text":"","category":"section"},{"location":"api-reference.html#Memory-Management","page":"API Reference","title":"Memory Management","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"# Monitor memory usage\nusing Profile\n\n@profile fit!(regressor, epochs, population_size, x_data', y_data)\nProfile.print()\n\n# Force garbage collection\nGC.gc()","category":"page"},{"location":"api-reference.html#Configuration-Examples","page":"API Reference","title":"Configuration Examples","text":"","category":"section"},{"location":"api-reference.html#Basic-Configuration","page":"API Reference","title":"Basic Configuration","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"regressor = GepRegressor(3)\nfit!(regressor, 1000, 1000, x_data', y_data)","category":"page"},{"location":"api-reference.html#Advanced-Configuration","page":"API Reference","title":"Advanced Configuration","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"regressor = GepRegressor(\n    5;                                    # 5 input features\n    population_size = 2000,               # Large population\n    gene_count = 3,                       # 3 genes per chromosome\n    head_len = 8,                         # Longer expressions\n    entered_non_terminals = [:+, :-, :*, :/, :sin, :cos, :exp]\n)\n\nfit!(regressor, 1500, 2000, x_train', y_train;\n     x_test = x_test', \n     y_test = y_test,\n     loss_fun = \"rmse\")","category":"page"},{"location":"api-reference.html#Multi-Objective-Configuration","page":"API Reference","title":"Multi-Objective Configuration","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"regressor = GepRegressor(\n    3;\n    number_of_objectives = 2,\n    population_size = 1500,\n    gene_count = 2,\n    head_len = 6\n)\n\nfit!(regressor, 1000, 1500, loss_function=multi_objective_loss)","category":"page"},{"location":"api-reference.html#Physical-Dimensionality-Configuration","page":"API Reference","title":"Physical Dimensionality Configuration","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"feature_dims = Dict{Symbol,Vector{Float16}}(\n    :x1 => Float16[1, 0, 0, 0, 0, 0, 0],  # Mass\n    :x2 => Float16[0, 1, 0, 0, 0, 0, 0],  # Length\n    :x3 => Float16[0, 0, 1, 0, 0, 0, 0],  # Time\n)\n\nregressor = GepRegressor(\n    3;\n    considered_dimensions = feature_dims,\n    max_permutations_lib = 15000,\n    rounds = 8\n)\n\ntarget_dim = Float16[1, 1, -2, 0, 0, 0, 0]  # Force\n\nfit!(regressor, 1200, 1200, x_data', y_data;\n     target_dimension = target_dim)","category":"page"},{"location":"api-reference.html#Tensor-Regression-Configuration","page":"API Reference","title":"Tensor Regression Configuration","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"regressor = GepTensorRegressor(\n    5,                                    # 5 features\n    gene_count = 2,\t\t\t  # Inserting the number of genes\n    head_len = 5, \t\t\t  # Inserting the head_len for each gene\n    feature_names = [\"scalar1\", \"scalar2\", \"vector1\", \"vector2\", \"matrix1\"]\n)\n\nfit!(regressor, 150, 800, tensor_loss_function)","category":"page"},{"location":"api-reference.html#Version-Information","page":"API Reference","title":"Version Information","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"# Get package version\nusing Pkg\nPkg.status(\"GeneExpressionProgramming\")\n\n# Check for updates\nPkg.update(\"GeneExpressionProgramming\")","category":"page"},{"location":"api-reference.html#Debugging-and-Diagnostics","page":"API Reference","title":"Debugging and Diagnostics","text":"","category":"section"},{"location":"api-reference.html#Fitness-History","page":"API Reference","title":"Fitness History","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"# Access fitness evolution\nif hasfield(typeof(regressor), :fitness_history_)\n    history = regressor.fitness_history_\n    train_loss = [elem[1] for elem in history.train_loss]\n    plot(train_loss)\nend","category":"page"},{"location":"api-reference.html#Expression-Analysis","page":"API Reference","title":"Expression Analysis","text":"","category":"section"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"# Analyze best expressions\nfor (i, model) in enumerate(regressor.best_models_)\n    println(\"Model $i: $(model.compiled_function)\")\n    println(\"Fitness: $(model.fitness)\")\nend","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"This API reference provides comprehensive coverage of all public interfaces in GeneExpressionProgramming.jl. For additional examples and use cases, refer to the Examples.","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api-reference.html","page":"API Reference","title":"API Reference","text":"For the most up-to-date API documentation, always refer to the package source code and docstrings.","category":"page"},{"location":"installation.html#Installation-Guide","page":"Installation","title":"Installation Guide","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"This guide provides comprehensive instructions for installing GeneExpressionProgramming.jl and its dependencies across different platforms and environments.","category":"page"},{"location":"installation.html#Prerequisites","page":"Installation","title":"Prerequisites","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Before installing GeneExpressionProgramming.jl, ensure you have the following prerequisites:","category":"page"},{"location":"installation.html#Julia-Requirements","page":"Installation","title":"Julia Requirements","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Julia 1.6 or later: The package requires Julia version 1.6 or higher. You can download Julia from the official website.\nPackage Manager: Julia's built-in package manager (Pkg) is required for installation.","category":"page"},{"location":"installation.html#System-Requirements","page":"Installation","title":"System Requirements","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"The package has been tested on the following platforms:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Linux (Ubuntu 18.04+, CentOS 7+)\nmacOS (10.14+)\nWindows 10/11","category":"page"},{"location":"installation.html#Memory-and-Performance-Considerations","page":"Installation","title":"Memory and Performance Considerations","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"For optimal performance, we recommend:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Minimum RAM: 4 GB (8 GB recommended for large datasets)\nCPU: Multi-core processor recommended for parallel operations\nStorage: At least 1 GB free space for package dependencies","category":"page"},{"location":"installation.html#Installation-Methods","page":"Installation","title":"Installation Methods","text":"","category":"section"},{"location":"installation.html#Method-1:-Direct-Installation-from-GitHub-(Recommended)","page":"Installation","title":"Method 1: Direct Installation from GitHub (Recommended)","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"The most straightforward way to install GeneExpressionProgramming.jl is directly from the GitHub repository:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"using Pkg\nPkg.add(url=\"https://github.com/maxreiss123/GeneExpressionProgramming.jl.git\")","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"This method ensures you get the latest version with all recent updates and bug fixes.","category":"page"},{"location":"installation.html#Method-2:-Development-Installation","page":"Installation","title":"Method 2: Development Installation","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"If you plan to contribute to the package or need to modify the source code, you can install it in development mode:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"using Pkg\nPkg.develop(url=\"https://github.com/maxreiss123/GeneExpressionProgramming.jl.git\")","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"This creates a local copy of the repository in your Julia development directory, allowing you to make changes and test them immediately.","category":"page"},{"location":"installation.html#Method-3:-Local-Installation","page":"Installation","title":"Method 3: Local Installation","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"If you have downloaded the source code locally:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"using Pkg\nPkg.add(path=\"/path/to/GeneExpressionProgramming.jl\")","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Replace /path/to/GeneExpressionProgramming.jl with the actual path to your local copy.","category":"page"},{"location":"installation.html#Dependency-Installation","page":"Installation","title":"Dependency Installation","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"GeneExpressionProgramming.jl automatically installs its dependencies during the installation process. The main dependencies include:","category":"page"},{"location":"installation.html#Core-Dependencies","page":"Installation","title":"Core Dependencies","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"DynamicExpressions.jl: For fast symbolic expression evaluation\nFlux.jl: For tensor operations and neural network backends\nRandom: For random number generation and seeding\nStatistics: For statistical operations\nLinearAlgebra: For matrix operations","category":"page"},{"location":"installation.html#Optional-Dependencies","page":"Installation","title":"Optional Dependencies","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"For enhanced functionality, you may want to install additional packages:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"using Pkg\nPkg.add([\"Plots\", \"CSV\", \"DataFrames\", \"Tensors\"])","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Plots.jl: For visualization and plotting results\nCSV.jl: For reading CSV data files\nDataFrames.jl: For data manipulation and analysis\nTensors.jl: For advanced tensor operations","category":"page"},{"location":"installation.html#Verification","page":"Installation","title":"Verification","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"After installation, verify that the package works correctly:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"using GeneExpressionProgramming\n\n# Test basic functionality\nprintln(\"GeneExpressionProgramming.jl installed successfully!\")\n\n# Create a simple regressor to test\nregressor = GepRegressor(2)\nprintln(\"Basic regressor created: \", typeof(regressor))","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"If the installation was successful, you should see output confirming the package is working.","category":"page"},{"location":"installation.html#Troubleshooting","page":"Installation","title":"Troubleshooting","text":"","category":"section"},{"location":"installation.html#Common-Installation-Issues","page":"Installation","title":"Common Installation Issues","text":"","category":"section"},{"location":"installation.html#Issue-1:-Package-Not-Found","page":"Installation","title":"Issue 1: Package Not Found","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"ERROR: The following package names could not be resolved:\n * GeneExpressionProgramming (not found in project, manifest or registry)","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Solution: Ensure you're using the correct URL and that you have internet connectivity:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Pkg.add(url=\"https://github.com/maxreiss123/GeneExpressionProgramming.jl.git\")","category":"page"},{"location":"installation.html#Issue-2:-Dependency-Conflicts","page":"Installation","title":"Issue 2: Dependency Conflicts","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"ERROR: Unsatisfiable requirements detected for package...","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Solution: Update your Julia packages and try again:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"using Pkg\nPkg.update()\nPkg.add(url=\"https://github.com/maxreiss123/GeneExpressionProgramming.jl.git\")","category":"page"},{"location":"installation.html#Issue-3:-Compilation-Errors","page":"Installation","title":"Issue 3: Compilation Errors","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"If you encounter compilation errors during installation:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Update Julia: Ensure you're using Julia 1.6 or later\nClear package cache: \nusing Pkg\nPkg.gc()\nReinstall dependencies:\nPkg.instantiate()","category":"page"},{"location":"installation.html#Issue-4:-Permission-Errors-(Linux/macOS)","page":"Installation","title":"Issue 4: Permission Errors (Linux/macOS)","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"If you encounter permission errors:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Check Julia installation: Ensure Julia is properly installed with appropriate permissions\nUse local package directory: Consider installing packages in a local directory\nAvoid sudo: Never use sudo with Julia package operations","category":"page"},{"location":"installation.html#Performance-Optimization","page":"Installation","title":"Performance Optimization","text":"","category":"section"},{"location":"installation.html#Julia-Startup-Optimization","page":"Installation","title":"Julia Startup Optimization","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"To improve Julia startup time with GeneExpressionProgramming.jl:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Precompile packages:\nusing Pkg\nPkg.precompile()\nUse PackageCompiler.jl for creating system images:\nusing Pkg\nPkg.add(\"PackageCompiler\")\nusing PackageCompiler\ncreate_sysimage([\"GeneExpressionProgramming\"]; sysimage_path=\"gep_sysimage.so\")","category":"page"},{"location":"installation.html#Memory-Management","page":"Installation","title":"Memory Management","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"For large-scale problems:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Increase Julia heap size:\njulia --heap-size-hint=8G\nMonitor memory usage:\nusing Profile\n@profile your_gep_code()\nProfile.print()","category":"page"},{"location":"installation.html#Environment-Setup","page":"Installation","title":"Environment Setup","text":"","category":"section"},{"location":"installation.html#Jupyter-Notebook-Integration","page":"Installation","title":"Jupyter Notebook Integration","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"To use GeneExpressionProgramming.jl in Jupyter notebooks:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"using Pkg\nPkg.add(\"IJulia\")\nusing IJulia\nnotebook()","category":"page"},{"location":"installation.html#VS-Code-Integration","page":"Installation","title":"VS Code Integration","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"For development with VS Code:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Install the Julia extension for VS Code\nConfigure the Julia executable path\nUse the integrated REPL for interactive development","category":"page"},{"location":"installation.html#Docker-Environment","page":"Installation","title":"Docker Environment","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"For containerized environments, use the official Julia Docker image:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"FROM julia:1.8\n\nRUN julia -e 'using Pkg; Pkg.add(url=\"https://github.com/maxreiss123/GeneExpressionProgramming.jl.git\")'\n\nWORKDIR /app\nCOPY . .\n\nCMD [\"julia\", \"your_script.jl\"]","category":"page"},{"location":"installation.html#Next-Steps","page":"Installation","title":"Next Steps","text":"","category":"section"},{"location":"installation.html","page":"Installation","title":"Installation","text":"After successful installation, proceed to:","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Getting Started Guide - Learn basic usage patterns\nCore Concepts - Understand the theoretical foundations\nExamples - Explore practical applications","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"For additional help, consult GitHub repository.","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"","category":"page"},{"location":"installation.html","page":"Installation","title":"Installation","text":"Last updated: January 2025","category":"page"},{"location":"examples/physical-dimensions.html#Physical-Dimensionality-and-Semantic-Backpropagation","page":"Physical Dimensionality","title":"Physical Dimensionality and Semantic Backpropagation","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"One of the most innovative features of GeneExpressionProgramming.jl is its support for physical dimensionality constraints through semantic backpropagation. This feature ensures that evolved expressions respect physical units and dimensional homogeneity, making it particularly valuable for scientific and engineering applications where physical laws must be respected.","category":"page"},{"location":"examples/physical-dimensions.html#Understanding-Physical-Dimensionality","page":"Physical Dimensionality","title":"Understanding Physical Dimensionality","text":"","category":"section"},{"location":"examples/physical-dimensions.html#Dimensional-Analysis-Fundamentals","page":"Physical Dimensionality","title":"Dimensional Analysis Fundamentals","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Physical dimensionality is based on the principle that any valid physical equation must be dimensionally homogeneous - all terms must have the same physical dimensions. The fundamental dimensions in the International System of Units (SI) are:","category":"page"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Mass (M): kilogram [kg]\nLength (L): meter [m]  \nTime (T): second [s]\nTemperature (K): kelvin [K]\nElectric Current (I): ampere [A]\nAmount of Substance (N): mole [mol]\nLuminous Intensity (J): candela [cd]","category":"page"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Any physical quantity can be expressed as a combination of these fundamental dimensions. For example:","category":"page"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Velocity: [L T⁻¹]\nForce: [M L T⁻²]\nEnergy: [M L² T⁻²]\nElectric Charge: [A T]","category":"page"},{"location":"examples/physical-dimensions.html#Dimensional-Representation-in-GeneExpressionProgramming.jl","page":"Physical Dimensionality","title":"Dimensional Representation in GeneExpressionProgramming.jl","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Dimensions are represented as vectors where each component corresponds to a fundamental unit:","category":"page"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"# Dimension vector: [M, L, T, K, I, N, J]\nvelocity_dim = [0, 1, -1, 0, 0, 0, 0]    # [L T⁻¹]\nforce_dim = [1, 1, -2, 0, 0, 0, 0]       # [M L T⁻²]\nenergy_dim = [1, 2, -2, 0, 0, 0, 0]      # [M L² T⁻²]\ncharge_dim = [0, 0, 0, 1, 1, 0, 0]       # [A T]","category":"page"},{"location":"examples/physical-dimensions.html#Complete-Physical-Dimensionality-Example","page":"Physical Dimensionality","title":"Complete Physical Dimensionality Example","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"This example demonstrates how to use physical dimensionality constraints to discover the relationship for electric current density in superconductivity, based on the Feynman Lectures equation J = -ρqmA (Feynman III 21.20).","category":"page"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"using GeneExpressionProgramming\nusing Random\nusing CSV\nusing DataFrames\nusing Statistics\nusing Plots\n\n# Set random seed for reproducibility\nRandom.seed!(42)\n\nprintln(\"=== Physical Dimensionality Example ===\")\nprintln(\"Discovering: J = -ρqA/m (Feynman III 21.20)\")\nprintln(\"J: electric current density, ρ: charge density, q: electric charge\")\nprintln(\"m: mass, A: magnetic vector potential\")\nprintln()\n\n# Define target dimension for electric current density J\n# J has dimensions of [A m⁻²] = [0, -2, 0, 0,1, 0, 0]\ntarget_dim = Float16[0, -2, 0, 0, 1, 0, 0]  # Current density: A/m²\n\nprintln(\"Target dimension (J): [M, L, T, K, I, N, J] = $target_dim\")\nprintln(\"Physical meaning: Ampere per square meter [A m⁻²]\")\nprintln()\n\n# Define dimensions for input features\n# Based on the Feynman equation variables\nfeature_dims = Dict{Symbol,Vector{Float16}}(\n    :x1 => Float16[0, -3, 1, 0, 1, 0, 0],   # ρ (charge density)\n    :x2 => Float16[0, 0, 1, 0, 1, 0, 0],    # q (electric charge)\n    :x3 => Float16[1, 1, -2, 0, -1, 0, 0],  # A (magnetic vector potential)\n    :x4 => Float16[1, 0, 0, 0, 0, 0, 0],    # m (mass)\n)\n\nprintln(\"Feature dimensions:\")\nfor (var, dim) in feature_dims\n    println(\"  $var: $dim\")\nend\nprintln()\n\n# Physical interpretation of features\ninterpretations = Dict(\n    :x1 => \"ρ (charge density) [A s m⁻³]\",\n    :x2 => \"q (electric charge) [A s]\", \n    :x3 => \"A (magnetic vector potential) [kg m s⁻² A⁻¹]\",\n    :x4 => \"m (mass) [kg]\"\n)\n\nprintln(\"Physical interpretation:\")\nfor (var, interp) in interpretations\n    println(\"  $var: $interp\")\nend\nprintln()\n\n# Generate synthetic data based on the known relationship\n# J = -ρ * q * A / m \nn_samples = 500\n\nprintln(\"Generating synthetic data...\")\n\n# J = -ρ * q * A / m \nn_samples = 5000\n\n# Load the data from the file =>  rho_c_0,q,A_vec,m,target\ndata = Matrix(CSV.read(\"./paper/srsd/feynman-III.21.20\\$0.txt\", DataFrame))\nnum_cols = size(data, 2)\nx_train, y_train, x_test, y_test = train_test_split(data[:, 1:num_cols-1], data[:, num_cols]; consider=4)\n\nprintln(\"Training samples: $(length(y_train))\")\nprintln(\"Test samples: $(length(y_test))\")\nprintln()\n\n# Evolution parameters\nepochs = 1000\npopulation_size = 1000\nnum_features = num_cols - 1 \n\n# Create regressor with dimensional constraints\nregressor = GepRegressor(\n    num_features;\n    considered_dimensions=feature_dims,\n    max_permutations_lib=10000,  # Increase for more complex expressions\n    rounds=7                     # Tree depth for dimensional checking\n)\n\nprintln(\"Regressor configuration:\")\nprintln(\"  Features: $num_features\")\nprintln(\"  Dimensional constraints: enabled\")\nprintln(\"  Max permutations: 10000\")\nprintln(\"  Tree depth (rounds): 7\")\nprintln()\n\nprintln(\"Starting dimensionally-constrained evolution...\")\nstart_time = time()\n\n# Fit with dimensional constraints\nfit!(regressor, epochs, population_size, x_train', y_train; \n     x_test=x_test', y_test=y_test, \n     loss_fun=\"mse\", \n     target_dimension=target_dim)\n\ntraining_time = time() - start_time\nprintln(\"Training completed in $(round(training_time, digits=2)) seconds\")\nprintln()\n\n# Analyze results\nprintln(\"=== Results Analysis ===\")\n\n# Get the best model\nbest_model = regressor.best_models_[1]\nbest_expression = best_model.compiled_function\nbest_fitness = best_model.fitness\n\nprintln(\"Best evolved expression:\")\nprintln(\"  $best_expression\")\nprintln(\"  Fitness (MSE): $best_fitness\")\nprintln()\n\n# Make predictions\ntrain_predictions = regressor(x_train')\ntest_predictions = regressor(x_test')\n\n# Calculate comprehensive metrics\nfunction calculate_detailed_metrics(y_true, y_pred, dataset_name)\n    mse = mean((y_true .- y_pred).^2)\n    mae = mean(abs.(y_true .- y_pred))\n    rmse = sqrt(mse)\n    \n    # R² score\n    ss_res = sum((y_true .- y_pred).^2)\n    ss_tot = sum((y_true .- mean(y_true)).^2)\n    r2 = 1 - ss_res / ss_tot\n    \n    # Mean Absolute Percentage Error\n    mape = mean(abs.((y_true .- y_pred) ./ y_true)) * 100\n    \n    # Maximum error\n    max_error = maximum(abs.(y_true .- y_pred))\n    \n    println(\"$dataset_name Performance:\")\n    println(\"  MSE:        $(round(mse, sigdigits=6))\")\n    println(\"  MAE:        $(round(mae, sigdigits=6))\")\n    println(\"  RMSE:       $(round(rmse, sigdigits=6))\")\n    println(\"  R²:         $(round(r2, digits=6))\")\n    println(\"  MAPE:       $(round(mape, digits=2))%\")\n    println(\"  Max Error:  $(round(max_error, sigdigits=6))\")\n    println()\n    \n    return (mse=mse, mae=mae, rmse=rmse, r2=r2, mape=mape, max_error=max_error)\nend\n\ntrain_metrics = calculate_detailed_metrics(y_train, train_predictions, \"Training\")\ntest_metrics = calculate_detailed_metrics(y_test, test_predictions, \"Test\")\n\n# Visualization\nprintln(\"Creating visualizations...\")\n\n# 1. Prediction accuracy plot\np1 = scatter(y_train, train_predictions,\n             xlabel=\"Actual Current Density\",\n             ylabel=\"Predicted Current Density\", \n             title=\"Training Set: Actual vs Predicted\",\n             alpha=0.6,\n             color=:blue,\n             label=\"Training Data\",\n             markersize=3)\n\nscatter!(p1, y_test, test_predictions,\n         alpha=0.6,\n         color=:red,\n         label=\"Test Data\",\n         markersize=3)\n\n# Perfect prediction line\nall_y = vcat(y_train, y_test)\nall_pred = vcat(train_predictions, test_predictions)\nmin_val = min(minimum(all_y), minimum(all_pred))\nmax_val = max(maximum(all_y), maximum(all_pred))\n\nplot!(p1, [min_val, max_val], [min_val, max_val],\n      color=:black, linestyle=:dash, linewidth=2, label=\"Perfect Prediction\")\n\n# 2. Residual analysis\ntrain_residuals = y_train .- train_predictions\ntest_residuals = y_test .- test_predictions\n\np2 = scatter(train_predictions, train_residuals,\n             xlabel=\"Predicted Values\",\n             ylabel=\"Residuals\",\n             title=\"Residual Analysis\",\n             alpha=0.6,\n             color=:blue,\n             label=\"Training\",\n             markersize=3)\n\nscatter!(p2, test_predictions, test_residuals,\n         alpha=0.6,\n         color=:red,\n         label=\"Test\",\n         markersize=3)\n\nhline!(p2, [0], color=:black, linestyle=:dash, linewidth=2, label=\"Zero Line\")\n\n# 3. Feature importance analysis (correlation with target)\nfeature_names = [\"ρ\", \"q\", \"A\", \"m\"]\ncorrelations = [cor(x_train[:, i], y_train) for i in 1:4]\n\np3 = bar(feature_names, abs.(correlations),\n         xlabel=\"Features\",\n         ylabel=\"Absolute Correlation with Target\",\n         title=\"Feature Importance\",\n         color=:lightblue,\n         legend=false)\n\n\n# Combine plots\nfinal_plot = plot(p1, p2, p3, layout=(3,1), size=(1000, 800))\nsavefig(final_plot, \"physical_dimensionality_analysis.png\")\nprintln(\"Analysis plot saved as 'physical_dimensionality_analysis.png'\")\n\n# 5. Detailed comparison with true relationship\nprintln(\"=== Detailed Comparison with True Relationship ===\")\n\n# Calculate predictions using the true relationship\ntrue_predictions_train = -x_train[:, 1] .* x_train[:, 2] .* x_train[:, 3] ./ x_train[:, 4]\ntrue_predictions_test = -x_test[:, 1] .* x_test[:, 2] .* x_test[:, 3] ./ x_test[:, 4]\n\n# Compare evolved vs true relationship\ncomparison_plot = plot(layout=(1,2), size=(1000, 400))\n\n# Training comparison\nscatter!(comparison_plot[1], true_predictions_train, train_predictions,\n         xlabel=\"True Relationship Predictions\",\n         ylabel=\"Evolved Expression Predictions\",\n         title=\"Training: Evolved vs True Relationship\",\n         alpha=0.6,\n         color=:blue,\n         legend=false,\n         markersize=3)\n\n# Test comparison  \nscatter!(comparison_plot[2], true_predictions_test, test_predictions,\n         xlabel=\"True Relationship Predictions\", \n         ylabel=\"Evolved Expression Predictions\",\n         title=\"Test: Evolved vs True Relationship\",\n         alpha=0.6,\n         color=:red,\n         legend=false,\n         markersize=3)\n\n# Perfect agreement lines\nfor i in 1:2\n    min_val = i == 1 ? minimum(true_predictions_train) : minimum(true_predictions_test)\n    max_val = i == 1 ? maximum(true_predictions_train) : maximum(true_predictions_test)\n    plot!(comparison_plot[i], [min_val, max_val], [min_val, max_val],\n          color=:black, linestyle=:dash, linewidth=2)\nend\n\nsavefig(comparison_plot, \"evolved_vs_true_comparison.png\")\nprintln(\"Evolved vs true relationship comparison saved as 'evolved_vs_true_comparison.png'\")\n\n# Calculate agreement metrics\ntrain_agreement = cor(true_predictions_train, train_predictions)\ntest_agreement = cor(true_predictions_test, test_predictions)\n\nprintln(\"Agreement with true relationship:\")\nprintln(\"  Training correlation: $(round(train_agreement, digits=6))\")\nprintln(\"  Test correlation: $(round(test_agreement, digits=6))\")\nprintln()","category":"page"},{"location":"examples/physical-dimensions.html#Real-World-Applications","page":"Physical Dimensionality","title":"Real-World Applications","text":"","category":"section"},{"location":"examples/physical-dimensions.html#1.-Fluid-Dynamics","page":"Physical Dimensionality","title":"1. Fluid Dynamics","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"# Discovering relationships in fluid mechanics\n# Example: Pressure drop in pipe flow\n\nfeature_dims_fluid = Dict{Symbol,Vector{Float16}}(\n    :x1 => Float16[0, 0, 0, 0, 0, 0, 0],     # Friction Factor\n    :x2 => Float16[1, -3, 0, 0, 0, 0, 0],    # ρ (density)  \n    :x3 => Float16[0, 1, -1, 0, 0, 0, 0],    # v (velocity) \n    :x4 => Float16[0, 1, 0, 0, 0, 0, 0],     # L (length)  \n    :x5 => Float16[0, 1, 0, 0, 0, 0, 0]     # D (diameter)\n)\n\n# Target: Pressure [ kg m⁻¹ s⁻²]\ntarget_dim_fluid = Float16[1, -1, -2, 0, 0, 0, 0]  # ΔP (pressure drop)","category":"page"},{"location":"examples/physical-dimensions.html#2.-Heat-Transfer","page":"Physical Dimensionality","title":"2. Heat Transfer","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"# Discovering heat transfer\n# Example: Heat flux\n\nfeature_dims_heat_flux = Dict{Symbol,Vector{Float16}}(\n    :x1 => Float16[1, 0, -3, 0, -1, 0, 0],   # h (heat transfer coefficient) \n    :x2 => Float16[0, 0, 0, 0, 1, 0, 0],     # ΔT (temperature difference)\n)\n\n# Target: Heat flux q = h⋅ΔT [W m⁻²]\ntarget_dim_heat_flux = Float16[1, 0, -3, 0, 0, 0, 0]","category":"page"},{"location":"examples/physical-dimensions.html#3.-Electromagnetic-Theory","page":"Physical Dimensionality","title":"3. Electromagnetic Theory","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"# Discovering electromagnetic relationships\n# Example: Electromagnetic wave propagation\n\nfeature_dims_em = Dict{Symbol,Vector{Float16}}(\n    :x1 => Float16[0, 1, -1, 0, 0, 0, 0],    # c (speed of light) \n    :x2 => Float16[0, 0, -1, 0, 0, 0, 0],    # f (frequency)\n    :x3 => Float16[0, 1, 0, 0, 0, 0, 0],     # λ (wavelength)\n    :x4 => Float16[-1, -3, 4, 0, 2, 0, 0],   # ε₀ (permittivity)\n    :x5 => Float16[1, 1, -2, 0, -2, 0, 0],   # μ₀ (permeability)\n)\n\n# Target: Speed [m s⁻¹]\ntarget_dim_em = Float16[0, 1, -1, 0, 0, 0, 0]","category":"page"},{"location":"examples/physical-dimensions.html#Benefits-of-Dimensional-Constraints","page":"Physical Dimensionality","title":"Benefits of Dimensional Constraints","text":"","category":"section"},{"location":"examples/physical-dimensions.html#1.-Physical-Plausibility","page":"Physical Dimensionality","title":"1. Physical Plausibility","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Ensures evolved expressions make physical sense\nPrevents dimensionally inconsistent solutions\nMaintains scientific validity","category":"page"},{"location":"examples/physical-dimensions.html#2.-Search-Space-Reduction","page":"Physical Dimensionality","title":"2. Search Space Reduction","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Eliminates large portions of invalid solution space\nFocuses search on physically meaningful expressions\nImproves convergence efficiency","category":"page"},{"location":"examples/physical-dimensions.html#3.-Interpretability","page":"Physical Dimensionality","title":"3. Interpretability","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Results have clear physical meaning\nEasier to validate against known physics\nFacilitates scientific understanding","category":"page"},{"location":"examples/physical-dimensions.html#4.-Robustness","page":"Physical Dimensionality","title":"4. Robustness","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Reduces overfitting to specific datasets\nImproves generalization to new conditions\nIncreases confidence in extrapolation","category":"page"},{"location":"examples/physical-dimensions.html#Common-Challenges-and-Solutions","page":"Physical Dimensionality","title":"Common Challenges and Solutions","text":"","category":"section"},{"location":"examples/physical-dimensions.html#Challenge-1:-Complex-Dimensional-Relationships","page":"Physical Dimensionality","title":"Challenge 1: Complex Dimensional Relationships","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Problem: Some physical relationships involve complex dimensional dependencies Solution: Use hierarchical approach, break into simpler sub-problems","category":"page"},{"location":"examples/physical-dimensions.html#Challenge-2:-Dimensionless-Numbers","page":"Physical Dimensionality","title":"Challenge 2: Dimensionless Numbers","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Problem: Important physics often involves dimensionless groups Solution: Include dimensionless combinations as derived features","category":"page"},{"location":"examples/physical-dimensions.html#Challenge-3:-Unit-System-Consistency","page":"Physical Dimensionality","title":"Challenge 3: Unit System Consistency","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Problem: Mixed unit systems can cause confusion Solution: Standardize on SI units throughout analysis","category":"page"},{"location":"examples/physical-dimensions.html#Challenge-4:-Computational-Overhead","page":"Physical Dimensionality","title":"Challenge 4: Computational Overhead","text":"","category":"section"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Problem: Dimensional checking adds computational cost Solution: Optimize dimensional propagation algorithms, use caching","category":"page"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Physical dimensionality constraints in GeneExpressionProgramming.jl provide a powerful tool for discovering physically meaningful relationships while ensuring scientific validity. This feature makes the package particularly valuable for scientific and engineering applications where physical laws must be respected.","category":"page"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"","category":"page"},{"location":"examples/physical-dimensions.html","page":"Physical Dimensionality","title":"Physical Dimensionality","text":"Next: Tensor Regression","category":"page"},{"location":"examples/basic-regression.html#Basic-Symbolic-Regression","page":"Basic Regression","title":"Basic Symbolic Regression","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"This example demonstrates the fundamental usage of GeneExpressionProgramming.jl for symbolic regression tasks. We'll walk through a complete workflow from data generation to result analysis, providing detailed explanations and best practices along the way.","category":"page"},{"location":"examples/basic-regression.html#Problem-Setup","page":"Basic Regression","title":"Problem Setup","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"For this example, we'll work with a synthetic dataset where we know the true underlying function. This allows us to evaluate how well the algorithm can rediscover the known mathematical relationship. The target function we'll use is:","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"f(x₁, x₂) = x₁² + x₁ × x₂ - 2 × x₁ × x₂","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"This function combines polynomial terms and demonstrates the algorithm's ability to discover both quadratic relationships and interaction terms between variables.","category":"page"},{"location":"examples/basic-regression.html#Complete-Example-Code","page":"Basic Regression","title":"Complete Example Code","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"using GeneExpressionProgramming\nusing Random\nusing Statistics\nusing Plots\n\n# Set random seed for reproducibility\nRandom.seed!(42)\n\nprintln(\"=== Basic Symbolic Regression Example ===\")\nprintln(\"Target function: f(x₁, x₂) = x₁² + x₁×x₂ - 2×x₁×x₂\")\nprintln()\n\n# Problem parameters\nnumber_features = 2\nn_samples = 200\nnoise_level = 0.05\n\n# Generate training data\nprintln(\"Generating training data...\")\nx_train = randn(Float64, n_samples, number_features)\ny_train = @. x_train[:,1]^2 + x_train[:,1] * x_train[:,2] - 2 * x_train[:,1] * x_train[:,2]\n\n# Add noise to make the problem more realistic\ny_train += noise_level * randn(n_samples)\n\n# Generate separate test data\nn_test = 50\nx_test = randn(Float64, n_test, number_features)\ny_test = @. x_test[:,1]^2 + x_test[:,1] * x_test[:,2] - 2 * x_test[:,1] * x_test[:,2]\ny_test += noise_level * randn(n_test)\n\nprintln(\"Training samples: $n_samples\")\nprintln(\"Test samples: $n_test\")\nprintln(\"Noise level: $noise_level\")\nprintln()\n\n# Evolution parameters\nepochs = 1000\npopulation_size = 1000\n\nprintln(\"Evolution parameters:\")\nprintln(\"  Epochs: $epochs\")\nprintln(\"  Population size: $population_size\")\nprintln()\n\n# Create and configure the regressor\nregressor = GepRegressor(number_features)\n\nprintln(\"Starting evolution...\")\nstart_time = time()\n\n# Train the model\nfit!(regressor, epochs, population_size, x_train', y_train; loss_fun=\"mse\")\n\ntraining_time = time() - start_time\nprintln(\"Training completed in $(round(training_time, digits=2)) seconds\")\nprintln()\n\n# Make predictions\ntrain_predictions = regressor(x_train')\ntest_predictions = regressor(x_test')\n\n# Calculate performance metrics\nfunction calculate_metrics(y_true, y_pred)\n    mse = mean((y_true .- y_pred).^2)\n    mae = mean(abs.(y_true .- y_pred))\n    rmse = sqrt(mse)\n    \n    # R² score\n    ss_res = sum((y_true .- y_pred).^2)\n    ss_tot = sum((y_true .- mean(y_true)).^2)\n    r2 = 1 - ss_res / ss_tot\n    \n    return (mse=mse, mae=mae, rmse=rmse, r2=r2)\nend\n\ntrain_metrics = calculate_metrics(y_train, train_predictions)\ntest_metrics = calculate_metrics(y_test, test_predictions)\n\n# Display results\nprintln(\"=== Results ===\")\nprintln(\"Best evolved expression:\")\nprintln(\"  \", regressor.best_models_[1].compiled_function)\nprintln()\n\nprintln(\"Training Performance:\")\nprintln(\"  MSE:  $(round(train_metrics.mse, digits=6))\")\nprintln(\"  MAE:  $(round(train_metrics.mae, digits=6))\")\nprintln(\"  RMSE: $(round(train_metrics.rmse, digits=6))\")\nprintln(\"  R²:   $(round(train_metrics.r2, digits=6))\")\nprintln()\n\nprintln(\"Test Performance:\")\nprintln(\"  MSE:  $(round(test_metrics.mse, digits=6))\")\nprintln(\"  MAE:  $(round(test_metrics.mae, digits=6))\")\nprintln(\"  RMSE: $(round(test_metrics.rmse, digits=6))\")\nprintln(\"  R²:   $(round(test_metrics.r2, digits=6))\")\nprintln()\n\n# Fitness evolution plot\nif hasfield(typeof(regressor), :fitness_history_) && \n   !isnothing(regressor.fitness_history_) && \n   hasfield(typeof(regressor.fitness_history_), :train_loss)\n    \n    fitness_history = [elem[1] for elem in regressor.fitness_history_.train_loss]\n    \n    p1 = plot(1:length(fitness_history), fitness_history,\n              xlabel=\"Generation\",\n              ylabel=\"Best Fitness (MSE)\",\n              title=\"Evolution Progress\",\n              legend=false,\n              linewidth=2,\n              color=:blue)\n    \n    # Log scale for better visualization\n    p2 = plot(1:length(fitness_history), fitness_history,\n              xlabel=\"Generation\",\n              ylabel=\"Best Fitness (MSE)\",\n              title=\"Evolution Progress (Log Scale)\",\n              legend=false,\n              linewidth=2,\n              color=:blue,\n              yscale=:log10)\n    \n    plot(p1, p2, layout=(1,2), size=(800, 300))\n    savefig(\"fitness_evolution.png\")\n    println(\"Fitness evolution plot saved as 'fitness_evolution.png'\")\nend\n\n# Prediction accuracy plots\np3 = scatter(y_train, train_predictions,\n             xlabel=\"Actual Values\",\n             ylabel=\"Predicted Values\",\n             title=\"Training Set: Actual vs Predicted\",\n             alpha=0.6,\n             color=:blue,\n             label=\"Training Data\")\n\n# Add perfect prediction line\nmin_val = min(minimum(y_train), minimum(train_predictions))\nmax_val = max(maximum(y_train), maximum(train_predictions))\nplot!(p3, [min_val, max_val], [min_val, max_val],\n      color=:red, linestyle=:dash, linewidth=2, label=\"Perfect Prediction\")\n\np4 = scatter(y_test, test_predictions,\n             xlabel=\"Actual Values\",\n             ylabel=\"Predicted Values\",\n             title=\"Test Set: Actual vs Predicted\",\n             alpha=0.6,\n             color=:green,\n             label=\"Test Data\")\n\nplot!(p4, [min_val, max_val], [min_val, max_val],\n      color=:red, linestyle=:dash, linewidth=2, label=\"Perfect Prediction\")\n\nplot(p3, p4, layout=(1,2), size=(800, 300))\nsavefig(\"prediction_accuracy.png\")\nprintln(\"Prediction accuracy plot saved as 'prediction_accuracy.png'\")\n\n# Residual analysis\ntrain_residuals = y_train .- train_predictions\ntest_residuals = y_test .- test_predictions\n\np5 = scatter(train_predictions, train_residuals,\n             xlabel=\"Predicted Values\",\n             ylabel=\"Residuals\",\n             title=\"Training Residuals\",\n             alpha=0.6,\n             color=:blue,\n             label=\"Training\")\nhline!(p5, [0], color=:red, linestyle=:dash, linewidth=2, label=\"Zero Line\")\n\np6 = scatter(test_predictions, test_residuals,\n             xlabel=\"Predicted Values\",\n             ylabel=\"Residuals\",\n             title=\"Test Residuals\",\n             alpha=0.6,\n             color=:green,\n             label=\"Test\")\nhline!(p6, [0], color=:red, linestyle=:dash, linewidth=2, label=\"Zero Line\")\n\nplot(p5, p6, layout=(1,2), size=(800, 300))\nsavefig(\"residual_analysis.png\")\nprintln(\"Residual analysis plot saved as 'residual_analysis.png'\")\n\nprintln()\nprintln(\"=== Analysis Complete ===\")","category":"page"},{"location":"examples/basic-regression.html#Detailed-Code-Explanation","page":"Basic Regression","title":"Detailed Code Explanation","text":"","category":"section"},{"location":"examples/basic-regression.html#Data-Generation","page":"Basic Regression","title":"Data Generation","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"The example begins by generating synthetic data with a known mathematical relationship. This approach allows us to evaluate the algorithm's performance objectively since we know the ground truth.","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"x_train = randn(Float64, n_samples, number_features)\ny_train = @. x_train[:,1]^2 + x_train[:,1] * x_train[:,2] - 2 * x_train[:,1] * x_train[:,2]\ny_train += noise_level * randn(n_samples)","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"The input features are drawn from a standard normal distribution, which provides a good range of values for testing the algorithm. The target function combines:","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"A quadratic term: x₁²\nAn interaction term: x₁ × x₂\nA scaled interaction term: -2 × x₁ × x₂","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Adding noise makes the problem more realistic and tests the algorithm's robustness to measurement errors.","category":"page"},{"location":"examples/basic-regression.html#Regressor-Configuration","page":"Basic Regression","title":"Regressor Configuration","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"regressor = GepRegressor(number_features)","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"The GepRegressor is initialized with the number of input features. By default, it uses sensible parameter values that work well for most problems:","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Population size: 1000\nGene count: 2\nHead length: 7\nFunction set: Basic arithmetic operations (+, -, *, /)","category":"page"},{"location":"examples/basic-regression.html#Training-Process","page":"Basic Regression","title":"Training Process","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"fit!(regressor, epochs, population_size, x_train', y_train; loss_fun=\"mse\")","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"The training process uses the fit! function with:","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"epochs: Number of generations for evolution\npopulation_size: Number of individuals in each generation\nx_train': Transposed feature matrix (features as rows)\ny_train: Target values\nloss_fun: Loss function (\"mse\" for mean squared error)","category":"page"},{"location":"examples/basic-regression.html#Performance-Evaluation","page":"Basic Regression","title":"Performance Evaluation","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"The example includes comprehensive performance evaluation:","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"function calculate_metrics(y_true, y_pred)\n    mse = mean((y_true .- y_pred).^2)\n    mae = mean(abs.(y_true .- y_pred))\n    rmse = sqrt(mse)\n    \n    ss_res = sum((y_true .- y_pred).^2)\n    ss_tot = sum((y_true .- mean(y_true)).^2)\n    r2 = 1 - ss_res / ss_tot\n    \n    return (mse=mse, mae=mae, rmse=rmse, r2=r2)\nend","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"This function calculates multiple metrics:","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"MSE: Mean Squared Error - penalizes large errors more heavily\nMAE: Mean Absolute Error - robust to outliers\nRMSE: Root Mean Squared Error - in the same units as the target\nR²: Coefficient of determination - proportion of variance explained","category":"page"},{"location":"examples/basic-regression.html#Visualization-and-Analysis","page":"Basic Regression","title":"Visualization and Analysis","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"The example generates several plots for analysis:","category":"page"},{"location":"examples/basic-regression.html#1.-Fitness-Evolution","page":"Basic Regression","title":"1. Fitness Evolution","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Shows how the best fitness improves over generations, helping you understand convergence behavior.","category":"page"},{"location":"examples/basic-regression.html#2.-Prediction-Accuracy","page":"Basic Regression","title":"2. Prediction Accuracy","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Scatter plots comparing actual vs. predicted values, with a perfect prediction line for reference.","category":"page"},{"location":"examples/basic-regression.html#3.-Residual-Analysis","page":"Basic Regression","title":"3. Residual Analysis","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Plots residuals (prediction errors) against predicted values to identify patterns in the errors.","category":"page"},{"location":"examples/basic-regression.html#Parameter-Sensitivity","page":"Basic Regression","title":"Parameter Sensitivity","text":"","category":"section"},{"location":"examples/basic-regression.html#Population-Size-Effects","page":"Basic Regression","title":"Population Size Effects","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"# Small population (fast but limited exploration)\nregressor_small = GepRegressor(number_features)\nfit!(regressor_small, 500, 100, x_train', y_train; loss_fun=\"mse\")\n\n# Large population (thorough exploration but slower)\nregressor_large = GepRegressor(number_features)\nfit!(regressor_large, 200, 2000, x_train', y_train; loss_fun=\"mse\")","category":"page"},{"location":"examples/basic-regression.html#Function-Set-Customization","page":"Basic Regression","title":"Function Set Customization","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"# Extended function set\nregressor_extended = GepRegressor(number_features; \n                                 entered_non_terminals=[:+, :-, :*, :/, :sin, :cos, :exp])\nfit!(regressor_extended, epochs, population_size, x_train', y_train; loss_fun=\"mse\")","category":"page"},{"location":"examples/basic-regression.html#Common-Issues-and-Solutions","page":"Basic Regression","title":"Common Issues and Solutions","text":"","category":"section"},{"location":"examples/basic-regression.html#Issue-1:-Poor-Convergence","page":"Basic Regression","title":"Issue 1: Poor Convergence","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Symptoms: High MSE, low R², fitness plateaus early","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Solutions:","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Increase population size or number of generations\nAdjust mutation/crossover rates\nTry different random seeds\nCheck data quality and scaling","category":"page"},{"location":"examples/basic-regression.html#Issue-2:-Overfitting","page":"Basic Regression","title":"Issue 2: Overfitting","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Symptoms: Good training performance, poor test performance","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Solutions:","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Use cross-validation\nReduce expression complexity (shorter head length)\nAdd regularization through multi-objective optimization\nIncrease training data size","category":"page"},{"location":"examples/basic-regression.html#Issue-3:-Slow-Convergence","page":"Basic Regression","title":"Issue 3: Slow Convergence","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Symptoms: Fitness improves very slowly","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Solutions:","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Increase mutation rate for more exploration\nUse larger population size\nCheck for proper data scaling\nConsider different loss functions","category":"page"},{"location":"examples/basic-regression.html#Advanced-Variations","page":"Basic Regression","title":"Advanced Variations","text":"","category":"section"},{"location":"examples/basic-regression.html#Custom-Loss-Function","page":"Basic Regression","title":"Custom Loss Function","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"function custom_loss(y_true, y_pred)\n    # Huber loss (robust to outliers)\n    delta = 1.0\n    residual = abs.(y_true .- y_pred)\n    return mean(ifelse.(residual .<= delta, \n                       0.5 * residual.^2, \n                       delta * (residual .- 0.5 * delta)))\nend\n\n# Use custom loss function\nfit!(regressor, epochs, population_size, x_train', y_train; loss_fun=custom_loss)","category":"page"},{"location":"examples/basic-regression.html#Early-Stopping","page":"Basic Regression","title":"Early Stopping","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"function fit_with_early_stopping!(regressor, max_epochs, population_size, x_train, y_train;\n                                  patience=50, min_improvement=1e-6)\n    best_fitness = Inf\n    patience_counter = 0\n    \n    for epoch in 1:max_epochs\n        fit!(regressor, 1, population_size, x_train, y_train; loss_fun=\"mse\")\n        current_fitness = regressor.best_models_[1].fitness[1]\n        \n        if current_fitness < best_fitness - min_improvement\n            best_fitness = current_fitness\n            patience_counter = 0\n        else\n            patience_counter += 1\n        end\n        \n        if patience_counter >= patience\n            println(\"Early stopping at epoch $epoch\")\n            break\n        end\n    end\nend","category":"page"},{"location":"examples/basic-regression.html#Cross-Validation","page":"Basic Regression","title":"Cross-Validation","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"function cross_validate(X, y, k_folds=5)\n    n_samples = size(X, 1)\n    fold_size = div(n_samples, k_folds)\n    scores = Float64[]\n    \n    for fold in 1:k_folds\n        # Create train/validation split\n        val_start = (fold - 1) * fold_size + 1\n        val_end = min(fold * fold_size, n_samples)\n        \n        val_indices = val_start:val_end\n        train_indices = setdiff(1:n_samples, val_indices)\n        \n        X_train_fold = X[train_indices, :]\n        y_train_fold = y[train_indices]\n        X_val_fold = X[val_indices, :]\n        y_val_fold = y[val_indices]\n        \n        # Train model\n        regressor_fold = GepRegressor(size(X, 2))\n        fit!(regressor_fold, 500, 500, X_train_fold', y_train_fold; loss_fun=\"mse\")\n        \n        # Evaluate\n        y_pred_fold = regressor_fold(X_val_fold')\n        mse_fold = mean((y_val_fold .- y_pred_fold).^2)\n        push!(scores, mse_fold)\n        \n        println(\"Fold $fold MSE: $(round(mse_fold, digits=6))\")\n    end\n    \n    println(\"Mean CV MSE: $(round(mean(scores), digits=6)) ± $(round(std(scores), digits=6))\")\n    return scores\nend\n\n# Perform cross-validation\ncv_scores = cross_validate(x_train, y_train)","category":"page"},{"location":"examples/basic-regression.html#Best-Practices-Summary","page":"Basic Regression","title":"Best Practices Summary","text":"","category":"section"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Always use separate test data for unbiased performance evaluation\nMonitor convergence through fitness evolution plots\nAnalyze residuals to identify systematic errors\nUse appropriate metrics for your specific problem type\nConsider cross-validation for robust performance estimates\nVisualize results to gain insights into model behavior\nStart with simple parameters and gradually increase complexity","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"This basic example provides a solid foundation for understanding GeneExpressionProgramming.jl. The principles and techniques demonstrated here apply to more complex scenarios covered in the advanced examples.","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"","category":"page"},{"location":"examples/basic-regression.html","page":"Basic Regression","title":"Basic Regression","text":"Next: Multi-Objective Optimization","category":"page"},{"location":"index.html#GeneExpressionProgramming.jl-Documentation","page":"Home","title":"GeneExpressionProgramming.jl Documentation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Welcome to the comprehensive documentation for GeneExpressionProgramming.jl, a powerful Julia package for symbolic regression using Gene Expression Programming (GEP). This documentation provides everything you need to get started with symbolic regression, from basic concepts to advanced applications.","category":"page"},{"location":"index.html#What-is-GeneExpressionProgramming.jl?","page":"Home","title":"What is GeneExpressionProgramming.jl?","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"GeneExpressionProgramming.jl is a state-of-the-art symbolic regression package that combines the power of evolutionary algorithms with unique features like physical dimensionality constraints and tensor regression capabilities. It's designed for researchers, engineers, and data scientists who need to discover mathematical relationships in their data while maintaining physical plausibility and interpretability.","category":"page"},{"location":"index.html#Key-Features","page":"Home","title":"Key Features","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Gene Expression Programming: Advanced evolutionary algorithm for symbolic regression  \nMulti-Objective Optimization: Balance accuracy, complexity, and other objectives using NSGA-II  \nPhysical Dimensionality: Ensure dimensional consistency in evolved expressions  \nTensor Regression: Work with vector and matrix data using Flux.jl backend  \nHigh Performance: Optimized Julia implementation with parallel processing support  \nScientific Applications: Specialized features for physics, engineering, and scientific computing  ","category":"page"},{"location":"index.html#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Get up and running in minutes:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/maxreiss123/GeneExpressionProgramming.jl.git\")\n\nusing GeneExpressionProgramming\nusing Random\n\n# Generate sample data\nRandom.seed!(42)\nx_data = randn(100, 2)\ny_data = @. x_data[:,1]^2 + x_data[:,2]\n\n# Create and train regressor\nregressor = GepRegressor(2)\nfit!(regressor, 1000, 1000, x_data', y_data; loss_fun=\"mse\")\n\n# View the discovered expression\nprintln(regressor.best_models_[1].compiled_function)\n# Output: x1 * x1 + x2","category":"page"},{"location":"index.html#Documentation-Structure","page":"Home","title":"Documentation Structure","text":"","category":"section"},{"location":"index.html#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Installation - Install the package and dependencies\nGetting Started - Your first symbolic regression project\nCore Concepts - Understand the theory behind GEP","category":"page"},{"location":"index.html#User-Guide","page":"Home","title":"User Guide","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"API Reference - Complete function and type documentation","category":"page"},{"location":"index.html#Examples-and-Tutorials","page":"Home","title":"Examples and Tutorials","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Basic Regression - Fundamental symbolic regression workflow\nMulti-Objective Optimization - Balance multiple objectives\nPhysical Dimensionality - Enforce dimensional consistency\nTensor Regression - Work with vector and matrix data","category":"page"},{"location":"index.html#Why-Choose-GeneExpressionProgramming.jl?","page":"Home","title":"Why Choose GeneExpressionProgramming.jl?","text":"","category":"section"},{"location":"index.html#Unique-Advantages","page":"Home","title":"Unique Advantages","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Physical Dimensionality: Unlike other symbolic regression packages, GeneExpressionProgramming.jl can enforce physical unit consistency, ensuring that evolved expressions respect fundamental physical laws. This is crucial for scientific and engineering applications.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Multi-Objective Optimization: Built-in support for balancing multiple objectives like accuracy vs. complexity using the proven NSGA-II algorithm. This helps you find interpretable models that don't overfit.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Tensor Regression: Native support for vector and matrix data through integration with Flux.jl, enabling discovery of relationships involving geometric and tensor quantities.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Scientific Rigor: Developed by researchers for researchers, with features specifically designed for scientific computing and discovery.","category":"page"},{"location":"index.html#Performance-Benefits","page":"Home","title":"Performance Benefits","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Julia Performance: Leverages Julia's speed for fast evolution and expression evaluation\nParallel Processing: Automatic parallelization across available CPU cores\nMemory Efficiency: Optimized data structures and algorithms\nScalability: Handles large datasets and complex expressions","category":"page"},{"location":"index.html#Ease-of-Use","page":"Home","title":"Ease of Use","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Simple API: Intuitive interface that's easy to learn\nComprehensive Documentation: Extensive examples and tutorials\nActive Development: Regular updates and community support\nIntegration: Works well with the Julia ML ecosystem","category":"page"},{"location":"index.html#Research-Foundation","page":"Home","title":"Research Foundation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"GeneExpressionProgramming.jl is based on cutting-edge research in symbolic regression and evolutionary computation. The package implements novel techniques for constraining genetic symbolic regression via semantic backpropagation, as described in:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Reissmann, M., Fang, Y., Ooi, A. S. H., & Sandberg, R. D. (2025). Constraining genetic symbolic regression via semantic backpropagation. Genetic Programming and Evolvable Machines, 26(1), 12.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"This research introduces innovative methods for ensuring that evolved expressions respect physical constraints and dimensional consistency, making the package particularly valuable for scientific applications.","category":"page"},{"location":"index.html#Community-and-Support","page":"Home","title":"Community and Support","text":"","category":"section"},{"location":"index.html#Getting-Help","page":"Home","title":"Getting Help","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Documentation: Start with this comprehensive documentation\nGitHub Issues: Report bugs and request features\nDiscussions: Ask questions and share experiences","category":"page"},{"location":"index.html#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"We welcome contributions from the community! Whether you're:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Fixing bugs or adding features\nImproving documentation\nSharing examples and tutorials\nProviding feedback and suggestions","category":"page"},{"location":"index.html#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"If you use GeneExpressionProgramming.jl in your research, please cite:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"@article{Reissmann2025,\n  author   = {Maximilian Reissmann and Yuan Fang and Andrew S. H. Ooi and Richard D. Sandberg},\n  title    = {Constraining Genetic Symbolic Regression via Semantic Backpropagation},\n  journal  = {Genetic Programming and Evolvable Machines},\n  year     = {2025},\n  volume   = {26},\n  number   = {1},\n  pages    = {12},\n  doi      = {10.1007/s10710-025-09510-z},\n  url      = {https://doi.org/10.1007/s10710-025-09510-z}\n}","category":"page"},{"location":"index.html#Version-Information","page":"Home","title":"Version Information","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"This documentation covers GeneExpressionProgramming.jl version 0.3.0 and later. The package is actively developed, with regular updates and improvements. Check the GitHub repository for the latest version and release notes.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"","category":"page"},{"location":"examples/multi-objective.html#Multi-Objective-Optimization","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Multi-objective optimization is a powerful feature of GeneExpressionProgramming.jl that allows you to balance multiple competing objectives simultaneously. This is particularly useful in symbolic regression where you often want to find expressions that are both accurate and simple, or when you need to optimize for multiple performance criteria.","category":"page"},{"location":"examples/multi-objective.html#Understanding-Multi-Objective-Optimization","page":"Multi-Objective Optimization","title":"Understanding Multi-Objective Optimization","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"In traditional single-objective optimization, the goal is to find the single best solution according to one criterion (e.g., minimizing prediction error). However, real-world problems often involve trade-offs between multiple objectives:","category":"page"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Accuracy vs. Complexity: More complex expressions may fit data better but are harder to interpret\nTraining vs. Validation Performance: Avoiding overfitting while maintaining good training performance\nSpeed vs. Accuracy: Faster evaluation vs. more precise predictions\nRobustness vs. Precision: Stable performance across different conditions vs. optimal performance in specific scenarios","category":"page"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Multi-objective optimization using NSGA-II (Non-dominated Sorting Genetic Algorithm II) finds a set of Pareto-optimal solutions, allowing you to choose the best trade-off for your specific needs.","category":"page"},{"location":"examples/multi-objective.html#Complete-Multi-Objective-Example","page":"Multi-Objective Optimization","title":"Complete Multi-Objective Example","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"using GeneExpressionProgramming\nusing Random\nusing Statistics\nusing Plots\n\n# Set random seed for reproducibility\nRandom.seed!(123)\n\nprintln(\"=== Multi-Objective Optimization Example ===\")\nprintln(\"Objectives: Minimize MSE and Expression Complexity\")\nprintln()\n\n# Problem setup\nnumber_features = 2\nn_samples = 300\nnoise_level = 0.1\n\n# Generate more complex synthetic data\nprintln(\"Generating synthetic data...\")\nx_data = 4 * (rand(Float64, n_samples, number_features) .- 0.5)  # Range [-2, 2]\n\n# Complex target function with multiple terms\ny_data = @. x_data[:,1]^3 - 2*x_data[:,1]^2*x_data[:,2] + \n            x_data[:,1]*x_data[:,2]^2 + 0.5*sin(x_data[:,1]) - \n            0.3*cos(x_data[:,2]) + x_data[:,1] + x_data[:,2]\n\n# Add noise\ny_data += noise_level * randn(n_samples)\n\nprintln(\"Data generated with $(n_samples) samples\")\nprintln(\"Target function: x₁³ - 2x₁²x₂ + x₁x₂² + 0.5sin(x₁) - 0.3cos(x₂) + x₁ + x₂\")\nprintln(\"Noise level: $(noise_level)\")\nprintln()\n\n# Split data for validation\ntrain_ratio = 0.8\nn_train = round(Int, n_samples * train_ratio)\ntrain_indices = 1:n_train\nval_indices = (n_train+1):n_samples\n\nx_train = x_data[train_indices, :]\ny_train = y_data[train_indices]\nx_val = x_data[val_indices, :]\ny_val = y_data[val_indices]\n\nprintln(\"Training samples: $(length(y_train))\")\nprintln(\"Validation samples: $(length(y_val))\")\nprintln()\n\n# Evolution parameters\nepochs = 1000\npopulation_size = 1000\n\n# Create multi-objective regressor\nregressor = GepRegressor(number_features; number_of_objectives=2, entered_non_terminals=[:+,:*,:-,:sin,:cos])\n\nprintln(\"Multi-objective evolution parameters:\")\nprintln(\"  Epochs: $epochs\")\nprintln(\"  Population size: $population_size\")\nprintln(\"  Objectives: 2 (MSE + Complexity)\")\nprintln()\n\n# Define custom multi-objective loss function\n@inline function multi_objective_loss(elem, validate::Bool)\n    if isnan(mean(elem.fitness)) || validate\n        # Get the compiled expression\n        model = elem.compiled_function\n        \n        try\n            # Calculate predictions\n            y_pred_train = model(x_train')\n            y_pred_val = model(x_val')\n            \n            # Check for invalid predictions\n            if !all(isfinite.(y_pred_train)) || !all(isfinite.(y_pred_val))\n                elem.fitness = (typemax(Float64), typemax(Float64))\n                return\n            end\n            \n            # Objective 1: Mean Squared Error on validation set\n            mse_val = mean((y_val .- y_pred_val).^2)\n            \n            # Objective 2: Expression complexity (number of nodes)\n            # This is a proxy for interpretability\n            complexity = length(elem.expression_raw)  # Simple complexity measure\n            \n            # Store both objectives (both to be minimized)\n            elem.fitness = (mse_val, Float64(complexity))\n            \n        catch e\n            # Handle evaluation errors\n            elem.fitness = (typemax(Float64), typemax(Float64))\n        end\n    end\nend\n\nprintln(\"Starting multi-objective evolution...\")\nstart_time = time()\n\n# Train with custom multi-objective loss\nfit!(regressor, epochs, population_size, multi_objective_loss, hof=20)\n\ntraining_time = time() - start_time\nprintln(\"Training completed in $(round(training_time, digits=2)) seconds\")\nprintln()\n\n# Analyze Pareto front\nprintln(\"=== Pareto Front Analysis ===\")\npareto_solutions = regressor.best_models_\n\nprintln(\"Found $(length(pareto_solutions)) Pareto-optimal solutions\")\nprintln()\n\n# Extract objectives for all solutions\nmse_values = [sol.fitness[1] for sol in pareto_solutions]\ncomplexity_values = [sol.fitness[2] for sol in pareto_solutions]\n\n# Sort by complexity for better presentation\nsorted_indices = sortperm(complexity_values)\nsorted_solutions = pareto_solutions[sorted_indices]\nsorted_mse = mse_values[sorted_indices]\nsorted_complexity = complexity_values[sorted_indices]\n\nprintln(\"Pareto-optimal solutions (sorted by complexity):\")\nprintln(\"Index | Complexity | MSE      | Expression\")\nprintln(\"------|------------|----------|------------------------------------------\")\n\nfor (i, (sol, mse, comp)) in enumerate(zip(sorted_solutions, sorted_mse, sorted_complexity))\n    expr_str = string(sol.compiled_function)\n    # Truncate long expressions for display\n    if length(expr_str) > 40\n        expr_str = expr_str[1:37] * \"...\"\n    end\n    println(\"$(lpad(i, 5)) | $(lpad(round(Int, comp), 10)) | $(lpad(round(mse, digits=6), 8)) | $expr_str\")\nend\nprintln()\n\n# Select interesting solutions for detailed analysis\nsimplest_idx = 1  # Simplest expression\nmost_accurate_idx = length(sorted_solutions)  # Most accurate\nmiddle_idx = div(length(sorted_solutions), 2)  # Middle trade-off\n\nselected_indices = [simplest_idx, middle_idx, most_accurate_idx]\nselected_labels = [\"Simplest\", \"Balanced\", \"Most Accurate\"]\n\nprintln(\"=== Detailed Analysis of Selected Solutions ===\")\n\ndetailed_results = []\nfor (idx, label) in zip(selected_indices, selected_labels)\n    sol = sorted_solutions[idx]\n    model = sol.compiled_function\n    \n    # Calculate comprehensive metrics\n    train_pred = model(x_train')\n    val_pred = model(x_val')\n    \n    train_mse = mean((y_train .- train_pred).^2)\n    val_mse = mean((y_val .- val_pred).^2)\n    \n    train_r2 = 1 - sum((y_train .- train_pred).^2) / sum((y_train .- mean(y_train)).^2)\n    val_r2 = 1 - sum((y_val .- val_pred).^2) / sum((y_val .- mean(y_val)).^2)\n    \n    complexity = sorted_complexity[idx]\n    \n    println(\"$label Solution:\")\n    println(\"  Expression: $model\")\n    println(\"  Complexity: $(round(Int, complexity))\")\n    println(\"  Training MSE: $(round(train_mse, digits=6))\")\n    println(\"  Validation MSE: $(round(val_mse, digits=6))\")\n    println(\"  Training R²: $(round(train_r2, digits=4))\")\n    println(\"  Validation R²: $(round(val_r2, digits=4))\")\n    println()\n    \n    push!(detailed_results, (\n        label=label,\n        model=model,\n        complexity=complexity,\n        train_mse=train_mse,\n        val_mse=val_mse,\n        train_r2=train_r2,\n        val_r2=val_r2,\n        train_pred=train_pred,\n        val_pred=val_pred\n    ))\nend\n\n# Visualization\nprintln(\"Creating visualizations...\")\n\n# 1. Pareto front plot\np1 = scatter(sorted_complexity, sorted_mse,\n             xlabel=\"Expression Complexity\",\n             ylabel=\"Validation MSE\",\n             title=\"Pareto Front: Accuracy vs Complexity\",\n             legend=false,\n             alpha=0.7,\n             color=:blue,\n             markersize=4)\n\n# Highlight selected solutions\nfor (idx, label) in zip(selected_indices, selected_labels)\n    scatter!(p1, [sorted_complexity[idx]], [sorted_mse[idx]],\n             color=:red, markersize=8, alpha=0.8,\n             label=label)\nend\n\n# Add annotations for selected points\nfor (idx, label) in zip(selected_indices, selected_labels)\n    annotate!(p1, sorted_complexity[idx], sorted_mse[idx] + 0.05 * maximum(sorted_mse),\n              text(label, 8, :center))\nend\n\n# 2. Prediction accuracy comparison\np2 = plot(layout=(1,3), size=(1200, 300))\n\nfor (i, result) in enumerate(detailed_results)\n    # Combine train and validation data for plotting\n    all_true = vcat(y_train, y_val)\n    all_pred = vcat(result.train_pred, result.val_pred)\n    \n    scatter!(p2[i], all_true, all_pred,\n             xlabel=\"Actual Values\",\n             ylabel=\"Predicted Values\",\n             title=\"$(result.label)\\nR² = $(round(result.val_r2, digits=3))\",\n             alpha=0.6,\n             legend=false)\n    \n    # Perfect prediction line\n    min_val = min(minimum(all_true), minimum(all_pred))\n    max_val = max(maximum(all_true), maximum(all_pred))\n    plot!(p2[i], [min_val, max_val], [min_val, max_val],\n          color=:red, linestyle=:dash, linewidth=2)\nend\n\n# 3. Trade-off analysis\np3 = plot(1:length(sorted_solutions), sorted_mse,\n          xlabel=\"Solution Index (by complexity)\",\n          ylabel=\"Validation MSE\",\n          title=\"MSE vs Solution Complexity Rank\",\n          legend=false,\n          linewidth=2,\n          color=:blue)\n\n# Mark selected solutions\nfor (idx, label) in zip(selected_indices, selected_labels)\n    scatter!(p3, [idx], [sorted_mse[idx]],\n             color=:red, markersize=8,\n             label=label)\nend\n\n# 4. Complexity distribution\np4 = histogram(sorted_complexity,\n               xlabel=\"Expression Complexity\",\n               ylabel=\"Number of Solutions\",\n               title=\"Distribution of Solution Complexity\",\n               bins=20,\n               alpha=0.7,\n               color=:lightblue,\n               legend=false)\n\n# Combine all plots\nfinal_plot = plot(p1, p3, p2, p4, layout=(2,2), size=(1000, 800))\nsavefig(final_plot, \"multi_objective_analysis.png\")\nprintln(\"Multi-objective analysis plot saved as 'multi_objective_analysis.png'\")\n\n# 5. Detailed comparison of selected solutions\ncomparison_plot = plot(layout=(1,3), size=(1200, 400))\n\nfor (i, result) in enumerate(detailed_results)\n    scatter!(comparison_plot[i], y_val, result.val_pred,\n             xlabel=\"Actual Values\",\n             ylabel=\"Predicted Values\",\n             title=\"$(result.label) (Complexity: $(round(Int, result.complexity)))\",\n             alpha=0.7,\n             legend=false,\n             color=[:blue, :green, :orange][i])\n    \n    # Perfect prediction line\n    min_val = min(minimum(y_val), minimum(result.val_pred))\n    max_val = max(maximum(y_val), maximum(result.val_pred))\n    plot!(comparison_plot[i], [min_val, max_val], [min_val, max_val],\n          color=:red, linestyle=:dash, linewidth=2)\nend\n\nsavefig(comparison_plot, \"solution_comparison.png\")\nprintln(\"Solution comparison plot saved as 'solution_comparison.png'\")\n\nprintln()\nprintln(\"=== Multi-Objective Optimization Complete ===\")\n\n# Decision support\nprintln(\"=== Decision Support ===\")\nprintln(\"Choose a solution based on your priorities:\")\nprintln()\n\nfor result in detailed_results\n    println(\"$(result.label):\")\n    println(\"  - Best for: \")\n    if result.label == \"Simplest\"\n        println(\"Interpretability and fast evaluation\")\n    elseif result.label == \"Balanced\"\n        println(\"Good trade-off between accuracy and simplicity\")\n    else\n        println(\"Maximum prediction accuracy\")\n    end\n    println(\"  - Validation R²: $(round(result.val_r2, digits=4))\")\n    println(\"  - Expression: $(result.model)\")\n    println()\nend\n\n# Recommend based on validation performance\nbest_val_r2_idx = argmax([r.val_r2 for r in detailed_results])\nprintln(\"Recommendation: For most applications, consider the '$(detailed_results[best_val_r2_idx].label)' solution\")\nprintln(\"as it provides the best validation performance.\")","category":"page"},{"location":"examples/multi-objective.html#Understanding-the-Results","page":"Multi-Objective Optimization","title":"Understanding the Results","text":"","category":"section"},{"location":"examples/multi-objective.html#Pareto-Front-Analysis","page":"Multi-Objective Optimization","title":"Pareto Front Analysis","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"The Pareto front represents the set of solutions where no other solution is better in all objectives simultaneously. Each point on the front represents a different trade-off between accuracy and complexity.","category":"page"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Key insights from the Pareto front:","category":"page"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Lower left: Simple but less accurate expressions\nUpper right: Complex but more accurate expressions\nElbow points: Often represent good trade-offs","category":"page"},{"location":"examples/multi-objective.html#Solution-Selection-Strategies","page":"Multi-Objective Optimization","title":"Solution Selection Strategies","text":"","category":"section"},{"location":"examples/multi-objective.html#1.-Simplest-Solution","page":"Multi-Objective Optimization","title":"1. Simplest Solution","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Use when: Interpretability is paramount\nAdvantages: Easy to understand, fast evaluation, less prone to overfitting\nDisadvantages: May sacrifice accuracy","category":"page"},{"location":"examples/multi-objective.html#2.-Balanced-Solution","page":"Multi-Objective Optimization","title":"2. Balanced Solution","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Use when: Need reasonable accuracy with moderate complexity\nAdvantages: Good compromise between interpretability and performance\nDisadvantages: May not excel in either dimension","category":"page"},{"location":"examples/multi-objective.html#3.-Most-Accurate-Solution","page":"Multi-Objective Optimization","title":"3. Most Accurate Solution","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Use when: Prediction accuracy is the primary concern\nAdvantages: Best predictive performance\nDisadvantages: May be complex and harder to interpret","category":"page"},{"location":"examples/multi-objective.html#Advanced-Multi-Objective-Techniques","page":"Multi-Objective Optimization","title":"Advanced Multi-Objective Techniques","text":"","category":"section"},{"location":"examples/multi-objective.html#Custom-Objective-Functions","page":"Multi-Objective Optimization","title":"Custom Objective Functions","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"# Example: Three-objective optimization\n@inline function three_objective_loss(elem, validate::Bool)\n    if isnan(mean(elem.fitness)) || validate\n        model = elem.compiled_function\n        \n        try\n            y_pred = model(x_train')\n            \n            # Objective 1: Training MSE\n            mse_train = mean((y_train .- y_pred).^2)\n            \n            # Objective 2: Expression complexity\n            complexity = count_nodes(model)  # Custom function to count nodes\n            \n            # Objective 3: Evaluation time (efficiency)\n            eval_time = @elapsed model(x_train')\n            \n            elem.fitness = (mse_train, Float64(complexity), eval_time)\n        catch\n            elem.fitness = (typemax(Float64), typemax(Float64), typemax(Float64))\n        end\n    end\nend","category":"page"},{"location":"examples/multi-objective.html#Weighted-Objectives","page":"Multi-Objective Optimization","title":"Weighted Objectives","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"# Convert multi-objective to single-objective with weights\nfunction weighted_objective(mse, complexity; w_mse=0.7, w_complexity=0.3)\n    # Normalize objectives to [0, 1] range\n    normalized_mse = mse / max_expected_mse\n    normalized_complexity = complexity / max_expected_complexity\n    \n    return w_mse * normalized_mse + w_complexity * normalized_complexity\nend","category":"page"},{"location":"examples/multi-objective.html#Constraint-Handling","page":"Multi-Objective Optimization","title":"Constraint Handling","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"@inline function constrained_loss(elem, validate::Bool)\n    if isnan(mean(elem.fitness)) || validate\n        model = elem.compiled_function\n        \n        try\n            y_pred = model(x_train')\n            complexity = length(string(model))\n            \n            # Hard constraint: complexity must be below threshold\n            if complexity > 100\n                elem.fitness = (typemax(Float64), typemax(Float64))\n                return\n            end\n            \n            mse = mean((y_train .- y_pred).^2)\n            elem.fitness = (mse, Float64(complexity))\n        catch\n            elem.fitness = (typemax(Float64), typemax(Float64))\n        end\n    end\nend","category":"page"},{"location":"examples/multi-objective.html#Performance-Considerations","page":"Multi-Objective Optimization","title":"Performance Considerations","text":"","category":"section"},{"location":"examples/multi-objective.html#Population-Size-for-Multi-Objective","page":"Multi-Objective Optimization","title":"Population Size for Multi-Objective","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Multi-objective optimization may requires larger populations than single-objective optimization:","category":"page"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"# Some test scenarios - population sizes\nsingle_objective_pop = 500\nmulti_objective_pop = 1000  # 2x for 2 objectives\nthree_objective_pop = 1500  # 3x for 3 objectives","category":"page"},{"location":"examples/multi-objective.html#Convergence-Monitoring","page":"Multi-Objective Optimization","title":"Convergence Monitoring","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"function plot_convergence(regressor)\n    # Extract fitness history for all objectives\n    if hasfield(typeof(regressor), :fitness_history_)\n        history = regressor.fitness_history_\n        \n        # Plot evolution of best solutions for each objective\n        generations = 1:length(history)\n        \n        p1 = plot(generations, [h[1] for h in history],\n                  xlabel=\"Generation\",\n                  ylabel=\"Best MSE\",\n                  title=\"Objective 1: MSE Evolution\",\n                  legend=false)\n        \n        p2 = plot(generations, [h[2] for h in history],\n                  xlabel=\"Generation\",\n                  ylabel=\"Best Complexity\",\n                  title=\"Objective 2: Complexity Evolution\",\n                  legend=false)\n        \n        plot(p1, p2, layout=(1,2))\n    end\nend","category":"page"},{"location":"examples/multi-objective.html#Real-World-Applications","page":"Multi-Objective Optimization","title":"Real-World Applications","text":"","category":"section"},{"location":"examples/multi-objective.html#1.-Financial-Modeling","page":"Multi-Objective Optimization","title":"1. Financial Modeling","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"# Objectives: Prediction accuracy + Model stability + Regulatory compliance\n@inline function financial_loss(elem, validate::Bool)\n    if isnan(mean(elem.fitness)) || validate\n        model = elem.compiled_function\n        \n        # Objective 1: Prediction accuracy\n        predictions = model(features')\n        accuracy = mean((returns .- predictions).^2)\n        \n        # Objective 2: Model stability (variance across time periods)\n        stability = var([model(period_data') for period_data in time_periods])\n        \n        # Objective 3: Regulatory compliance (expression interpretability)\n        compliance_score = interpretability_score(model)\n        \n        elem.fitness = (accuracy, stability, compliance_score)\n    end\nend","category":"page"},{"location":"examples/multi-objective.html#2.-Engineering-Design","page":"Multi-Objective Optimization","title":"2. Engineering Design","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"# Objectives: Performance + Cost + Safety margin\n@inline function engineering_loss(elem, validate::Bool)\n    if isnan(mean(elem.fitness)) || validate\n        model = elem.compiled_function\n        \n        # Objective 1: Performance (minimize error)\n        performance = mean((target_performance .- model(design_params')).^2)\n        \n        # Objective 2: Cost (complexity proxy)\n        cost = manufacturing_cost(model)\n        \n        # Objective 3: Safety (maximize safety margin)\n        safety_margin = -minimum_safety_factor(model)  # Negative for minimization\n        \n        elem.fitness = (performance, cost, safety_margin)\n    end\nend","category":"page"},{"location":"examples/multi-objective.html#3.-Scientific-Discovery","page":"Multi-Objective Optimization","title":"3. Scientific Discovery","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"# Objectives: Fit quality + Physical plausibility + Simplicity\n@inline function scientific_loss(elem, validate::Bool)\n    if isnan(mean(elem.fitness)) || validate\n        model = elem.compiled_function\n        \n        # Objective 1: Goodness of fit\n        fit_quality = mean((experimental_data .- model(conditions')).^2)\n        \n        # Objective 2: Physical plausibility\n        plausibility_penalty = physical_constraint_violations(model)\n        \n        # Objective 3: Occam's razor (simplicity)\n        simplicity = expression_complexity(model)\n        \n        elem.fitness = (fit_quality, plausibility_penalty, simplicity)\n    end\nend","category":"page"},{"location":"examples/multi-objective.html#Best-Practices-for-Multi-Objective-Optimization","page":"Multi-Objective Optimization","title":"Best Practices for Multi-Objective Optimization","text":"","category":"section"},{"location":"examples/multi-objective.html#1.-Objective-Design","page":"Multi-Objective Optimization","title":"1. Objective Design","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Ensure objectives are conflicting (otherwise single-objective is sufficient) - can be figured out by observing correlation of different targets\nScale objectives to similar ranges\nConsider the number of objectives (2-3 typically work best)","category":"page"},{"location":"examples/multi-objective.html#2.-Population-Management","page":"Multi-Objective Optimization","title":"2. Population Management","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Use larger populations for multi-objective problems\nMonitor diversity to ensure good Pareto front coverage\nConsider archive strategies for very long runs","category":"page"},{"location":"examples/multi-objective.html#3.-Solution-Selection","page":"Multi-Objective Optimization","title":"3. Solution Selection","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Use domain knowledge to guide selection\nConsider elbow points on the Pareto front\nValidate selected solutions on independent test data","category":"page"},{"location":"examples/multi-objective.html#4.-Visualization","page":"Multi-Objective Optimization","title":"4. Visualization","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Always visualize the Pareto front\nUse parallel coordinate plots for >2 objectives\nCreate decision support tools for stakeholders","category":"page"},{"location":"examples/multi-objective.html#5.-Computational-Efficiency","page":"Multi-Objective Optimization","title":"5. Computational Efficiency","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Profile objective function evaluation\nConsider approximation methods for expensive objectives\nUse parallel evaluation when possible","category":"page"},{"location":"examples/multi-objective.html#Common-Pitfalls-and-Solutions","page":"Multi-Objective Optimization","title":"Common Pitfalls and Solutions","text":"","category":"section"},{"location":"examples/multi-objective.html#Pitfall-1:-Dominated-Objectives","page":"Multi-Objective Optimization","title":"Pitfall 1: Dominated Objectives","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Problem: One objective dominates others, leading to poor trade-offs (can sometimes be observed in NSGA) Solution: Proper objective scaling and normalization","category":"page"},{"location":"examples/multi-objective.html#Pitfall-2:-Too-Many-Objectives","page":"Multi-Objective Optimization","title":"Pitfall 2: Too Many Objectives","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Problem: Curse of dimensionality in objective space Solution: Limit to 2-3 objectives or use specialized many-objective algorithms","category":"page"},{"location":"examples/multi-objective.html#Pitfall-3:-Poor-Diversity","page":"Multi-Objective Optimization","title":"Pitfall 3: Poor Diversity","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Problem: Solutions cluster in one region of the Pareto front Solution: Increase population size or mutation rate","category":"page"},{"location":"examples/multi-objective.html#Pitfall-4:-Expensive-Evaluation","page":"Multi-Objective Optimization","title":"Pitfall 4: Expensive Evaluation","text":"","category":"section"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Problem: Multi-objective evaluation is computationally expensive Solution: Use surrogate models, parallel evaluation, or approximation methods","category":"page"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"","category":"page"},{"location":"examples/multi-objective.html","page":"Multi-Objective Optimization","title":"Multi-Objective Optimization","text":"Next: Physical Dimensionality","category":"page"},{"location":"examples/tensor-regression.html#Tensor-Regression","page":"Tensor Regression","title":"Tensor Regression","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Hint: This topic is currently under development. The next update will include an improvement in performance and dimension handling. ","category":"page"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Tensor regression is an advanced feature of GeneExpressionProgramming.jl that enables symbolic regression with vector and matrix data. This capability is particularly valuable for applications in computational mechanics, fluid dynamics, computer vision, medical data from (MRI), and other fields where the relationships involve higher-dimensional mathematical objects.","category":"page"},{"location":"examples/tensor-regression.html#Understanding-Tensor-Operations","page":"Tensor Regression","title":"Understanding Tensor Operations","text":"","category":"section"},{"location":"examples/tensor-regression.html#Mathematical-Background","page":"Tensor Regression","title":"Mathematical Background","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Tensors are generalizations of scalars, vectors, and matrices to higher dimensions:","category":"page"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Rank 0 (Scalar): Single number\nRank 1 (Vector): Array of numbers with direction\nRank 2 (Matrix): 2D array of numbers\nRank n (Tensor): n-dimensional array","category":"page"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"In the context of symbolic regression, tensor operations allow us to discover relationships involving:","category":"page"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Vector addition and subtraction\nDot products and cross products\nMatrix multiplication and operations\nElement-wise operations on tensors\nContraction\nNorms and other tensor properties","category":"page"},{"location":"examples/tensor-regression.html#Tensor-Operations-in-GeneExpressionProgramming.jl","page":"Tensor Regression","title":"Tensor Operations in GeneExpressionProgramming.jl","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"The package supports tensor operations through the GepTensorRegressor, which uses Flux.jl as the backend for efficient tensor computations. This enables the evolution of expressions that can handle complex mathematical structures while maintaining the interpretability of symbolic expressions.","category":"page"},{"location":"examples/tensor-regression.html#Tensor-Regression-Example","page":"Tensor Regression","title":"Tensor Regression Example","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"This example demonstrates how to use tensor regression to discover relationships involving vector operations, simulating a scenario from computational fluid dynamics where we want to find a relationship between velocity vectors.","category":"page"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"using GeneExpressionProgramming\nusing Random\nusing Tensors\nusing LinearAlgebra\nusing Statistics\nusing Plots\n\n# Set random seed for reproducibility\nRandom.seed!(789)\n\nprintln(\"=== Tensor Regression Example ===\")\nprintln(\"Discovering vector relationships in 3D space\")\nprintln(\"Target: a = 0.5*u1 + x2*u2 + 2*u3\")\nprintln(\"where u1, u2, u3 are 3D vectors and x2 is a scalar\")\nprintln()\n\n# Problem setup\nnumber_features = 5  # x1, x2, u1, u2, u3\nn_samples = 1000\n\nprintln(\"Generating tensor data...\")\n\n# Generate scalar features\nx1 = [2.0 for _ in 1:n_samples]  # Constant scalar\nx2 = randn(n_samples)            # Variable scalar\n\n# Generate 3D vector features using Tensors.jl\nu1 = [Tensor{1,3}(randn(3)) for _ in 1:n_samples]\nu2 = [Tensor{1,3}(randn(3)) for _ in 1:n_samples]  \nu3 = [Tensor{1,3}(randn(3)) for _ in 1:n_samples]\n\n# Define the true relationship: a = 0.5*u1 + x2*u2 + 2*u3\na_true = [0.5 * u1[i] + x2[i] * u2[i] + 2.0 * u3[i] for i in 1:n_samples]\n\n# Add small amount of noise to make it more realistic\nnoise_level = 0.01\na_noisy = [a_true[i] + noise_level * Tensor{1,3}(randn(3)) for i in 1:n_samples]\n\nprintln(\"Data characteristics:\")\nprintln(\"  Samples: $n_samples\")\nprintln(\"  Scalar features: x1 (constant=2.0), x2 (variable)\")\nprintln(\"  Vector features: u1, u2, u3 (3D vectors)\")\nprintln(\"  Target: a (3D vector)\")\nprintln(\"  Noise level: $noise_level\")\nprintln()\n\n# Organize input data\ninputs = (x1, x2, u1, u2, u3)\n\nprintln(\"Input data structure:\")\nprintln(\"  x1: $(length(x1)) scalars\")\nprintln(\"  x2: $(length(x2)) scalars\") \nprintln(\"  u1: $(length(u1)) 3D vectors\")\nprintln(\"  u2: $(length(u2)) 3D vectors\")\nprintln(\"  u3: $(length(u3)) 3D vectors\")\nprintln()\n\n# Create tensor regressor\nregressor = GepTensorRegressor(\n    number_features,\n    gene_count=2,           # Number of genes (complexity control)\n    head_len=3;             # Head length (expression depth control)\n    feature_names=[\"x1\", \"x2\", \"U1\", \"U2\", \"U3\"]  # Names for interpretability\n)\n\nprintln(\"Tensor regressor configuration:\")\nprintln(\"  Features: $number_features\")\nprintln(\"  Gene count: 2\")\nprintln(\"  Head length: 3\")\nprintln()\n\n# Define custom loss function for tensor regression\n@inline function tensor_loss(elem, validate::Bool)\n    if isnan(mean(elem.fitness)) || validate\n        try\n            # Get the compiled model\n            model = elem.compiled_function\n            \n            # Make predictions\n            a_pred = model(inputs)\n            \n            # Check for valid predictions\n            if !isfinite(norm(a_pred)) || size(a_pred) != size(a_noisy)\n                elem.fitness = (typemax(Float64),)\n                return\n            end\n            \n            # Check individual vector sizes\n            if length(a_pred) != length(a_noisy) || \n               (length(a_pred) > 0 && size(a_pred[1]) != size(a_noisy[1]))\n                elem.fitness = (typemax(Float64),)\n                return\n            end\n            \n            # Calculate loss as norm of difference\n            total_error = 0.0\n            for i in 1:length(a_noisy)\n                error_vec = a_pred[i] - a_noisy[i]\n                total_error += norm(error_vec)^2\n            end\n            \n            loss = total_error / length(a_noisy)  # Mean squared error\n            elem.fitness = (loss,)\n            \n        catch e\n            # Handle any evaluation errors\n            elem.fitness = (typemax(Float64),)\n        end\n    end\nend\n\n# Evolution parameters\nepochs = 100  # Reduced for tensor regression (computationally intensive)\npopulation_size = 1000\n\nprintln(\"Evolution parameters:\")\nprintln(\"  Epochs: $epochs\")\nprintln(\"  Population size: $population_size\")\nprintln(\"  Loss function: Custom tensor MSE\")\nprintln()\n\nprintln(\"Starting tensor evolution...\")\nprintln(\"Note: Tensor operations are computationally intensive\")\nstart_time = time()\n\n# Train the tensor regressor\nfit!(regressor, epochs, population_size, tensor_loss)\n\ntraining_time = time() - start_time\nprintln(\"Training completed in $(round(training_time, digits=2)) seconds\")\nprintln()\n\n# Analyze results\nprintln(\"=== Results Analysis ===\")\n\n# Get the best evolved solution\nbest_solution = regressor.best_models_[1]\nbest_model = best_solution.compiled_function\nbest_fitness = best_solution.fitness[1]\n\nprintln(\"Best evolved expression:\")\nprint_karva_strings(best_solution)  # Print in Karva notation\nprintln()\nprintln(\"Best fitness (MSE): $best_fitness\")\nprintln()\n\n# Make predictions with the best model\nprintln(\"Making predictions...\")\ntry\n    a_pred = best_model(inputs)\n    \n    # Calculate comprehensive metrics\n    println(\"Prediction analysis:\")\n    println(\"  Predicted vectors: $(length(a_pred))\")\n    println(\"  Expected vectors: $(length(a_noisy))\")\n    \n    if length(a_pred) == length(a_noisy)\n        # Calculate vector-wise metrics\n        vector_errors = [norm(a_pred[i] - a_noisy[i]) for i in 1:length(a_noisy)]\n        component_errors = []\n        \n        for i in 1:length(a_noisy)\n            for j in 1:3  # 3D vectors\n                push!(component_errors, abs(a_pred[i][j] - a_noisy[i][j]))\n            end\n        end\n        \n        println(\"  Mean vector error: $(round(mean(vector_errors), digits=6))\")\n        println(\"  Max vector error: $(round(maximum(vector_errors), digits=6))\")\n        println(\"  Mean component error: $(round(mean(component_errors), digits=6))\")\n        println(\"  Max component error: $(round(maximum(component_errors), digits=6))\")\n        \n        # Calculate R² equivalent for vectors\n        total_variance = sum([norm(a_noisy[i] - mean(a_noisy))^2 for i in 1:length(a_noisy)])\n        residual_variance = sum([norm(a_pred[i] - a_noisy[i])^2 for i in 1:length(a_noisy)])\n        r2_equivalent = 1 - residual_variance / total_variance\n        \n        println(\"  R² equivalent: $(round(r2_equivalent, digits=6))\")\n        println()\n        \n        # Detailed component analysis\n        println(\"Component-wise analysis:\")\n        for comp in 1:3\n            true_comp = [a_noisy[i][comp] for i in 1:length(a_noisy)]\n            pred_comp = [a_pred[i][comp] for i in 1:length(a_pred)]\n            \n            comp_correlation = cor(true_comp, pred_comp)\n            comp_mse = mean((true_comp .- pred_comp).^2)\n            \n            println(\"  Component $comp:\")\n            println(\"    Correlation: $(round(comp_correlation, digits=4))\")\n            println(\"    MSE: $(round(comp_mse, digits=6))\")\n        end\n        println()\n        \n        # Visualization\n        println(\"Creating visualizations...\")\n        \n        # 1. Component-wise accuracy plots\n        p_components = plot(layout=(1,3), size=(1200, 300))\n        \n        for comp in 1:3\n            true_comp = [a_noisy[i][comp] for i in 1:length(a_noisy)]\n            pred_comp = [a_pred[i][comp] for i in 1:length(a_pred)]\n            \n            scatter!(p_components[comp], true_comp, pred_comp,\n                     xlabel=\"True Component $comp\",\n                     ylabel=\"Predicted Component $comp\",\n                     title=\"Component $comp Accuracy\",\n                     alpha=0.6,\n                     markersize=2,\n                     legend=false)\n            \n            # Perfect prediction line\n            min_val = min(minimum(true_comp), minimum(pred_comp))\n            max_val = max(maximum(true_comp), maximum(pred_comp))\n            plot!(p_components[comp], [min_val, max_val], [min_val, max_val],\n                  color=:red, linestyle=:dash, linewidth=2)\n        end\n        \n        savefig(p_components, \"tensor_component_accuracy.png\")\n        println(\"Component accuracy plot saved as 'tensor_component_accuracy.png'\")\n        \n        # 2. Vector magnitude comparison\n        true_magnitudes = [norm(a_noisy[i]) for i in 1:length(a_noisy)]\n        pred_magnitudes = [norm(a_pred[i]) for i in 1:length(a_pred)]\n        \n        p_magnitude = scatter(true_magnitudes, pred_magnitudes,\n                             xlabel=\"True Vector Magnitude\",\n                             ylabel=\"Predicted Vector Magnitude\",\n                             title=\"Vector Magnitude Accuracy\",\n                             alpha=0.6,\n                             markersize=3,\n                             legend=false)\n        \n        min_mag = min(minimum(true_magnitudes), minimum(pred_magnitudes))\n        max_mag = max(maximum(true_magnitudes), maximum(pred_magnitudes))\n        plot!(p_magnitude, [min_mag, max_mag], [min_mag, max_mag],\n              color=:red, linestyle=:dash, linewidth=2)\n        \n        savefig(p_magnitude, \"tensor_magnitude_accuracy.png\")\n        println(\"Vector magnitude plot saved as 'tensor_magnitude_accuracy.png'\")\n        \n        # 3. Error distribution\n        p_error = histogram(vector_errors,\n                           xlabel=\"Vector Error (L2 Norm)\",\n                           ylabel=\"Frequency\",\n                           title=\"Distribution of Vector Errors\",\n                           bins=30,\n                           alpha=0.7,\n                           legend=false)\n        \n        savefig(p_error, \"tensor_error_distribution.png\")\n        println(\"Error distribution plot saved as 'tensor_error_distribution.png'\")\n        \n        # 4. 3D visualization of a subset of vectors\n        n_viz = min(50, length(a_noisy))  # Visualize subset for clarity\n        indices = 1:n_viz\n        \n        p_3d = plot(layout=(1,2), size=(1000, 400))\n        \n        # True vectors\n        for i in indices\n            vec = a_noisy[i]\n            plot!(p_3d[1], [0, vec[1]], [0, vec[2]], [0, vec[3]],\n                  color=:blue, alpha=0.6, linewidth=1, legend=false)\n        end\n        plot!(p_3d[1], title=\"True Vectors (Sample)\", xlabel=\"X\", ylabel=\"Y\", zlabel=\"Z\")\n        \n        # Predicted vectors\n        for i in indices\n            vec = a_pred[i]\n            plot!(p_3d[2], [0, vec[1]], [0, vec[2]], [0, vec[3]],\n                  color=:red, alpha=0.6, linewidth=1, legend=false)\n        end\n        plot!(p_3d[2], title=\"Predicted Vectors (Sample)\", xlabel=\"X\", ylabel=\"Y\", zlabel=\"Z\")\n        \n        savefig(p_3d, \"tensor_3d_visualization.png\")\n        println(\"3D vector visualization saved as 'tensor_3d_visualization.png'\")\n        \n    else\n        println(\"Warning: Prediction size mismatch\")\n        println(\"Expected: $(length(a_noisy)) vectors\")\n        println(\"Got: $(length(a_pred)) vectors\")\n    end\n    \ncatch e\n    println(\"Error during prediction analysis: $e\")\nend\n\nprintln()\n\n# Compare with true relationship\nprintln(\"=== Comparison with True Relationship ===\")\nprintln(\"True relationship: a = 0.5*u1 + x2*u2 + 2*u3\")\n\n# Calculate predictions using true relationship\na_true_clean = [0.5 * u1[i] + x2[i] * u2[i] + 2.0 * u3[i] for i in 1:n_samples]\n\ntry\n    a_pred = best_model(inputs)\n    \n    if length(a_pred) == length(a_true_clean)\n        # Compare evolved expression with true relationship\n        agreement_errors = [norm(a_pred[i] - a_true_clean[i]) for i in 1:length(a_true_clean)]\n        mean_agreement_error = mean(agreement_errors)\n        \n        println(\"Agreement with true relationship:\")\n        println(\"  Mean error: $(round(mean_agreement_error, digits=6))\")\n        println(\"  Max error: $(round(maximum(agreement_errors), digits=6))\")\n        \n        # Component-wise agreement\n        for comp in 1:3\n            true_comp = [a_true_clean[i][comp] for i in 1:length(a_true_clean)]\n            pred_comp = [a_pred[i][comp] for i in 1:length(a_pred)]\n            agreement_corr = cor(true_comp, pred_comp)\n            println(\"  Component $comp correlation: $(round(agreement_corr, digits=4))\")\n        end\n    end\ncatch e\n    println(\"Error in true relationship comparison: $e\")\nend\n\nprintln()\n\n# Performance analysis\nprintln(\"=== Performance Analysis ===\")\nprintln(\"Computational considerations:\")\nprintln(\"  Training time: $(round(training_time, digits=2)) seconds\")\nprintln(\"  Time per epoch: $(round(training_time/epochs, digits=2)) seconds\")\nprintln(\"  Population evaluations: $(epochs * population_size)\")\n\n# Memory usage estimation\nvector_size = 3 * 8  # 3 components × 8 bytes per Float64\ntotal_vector_memory = n_samples * 3 * vector_size  # 3 vector features\nprintln(\"  Estimated vector memory: $(round(total_vector_memory/1024/1024, digits=2)) MB\")\n\nprintln()\n\n# Advanced tensor operations example\nprintln(\"=== Advanced Tensor Operations Example ===\")\nprintln(\"Demonstrating additional tensor capabilities...\")\n\n# Example with matrix operations\nprintln(\"Matrix operation example:\")\nn_matrix_samples = 100\n\n# Generate 2x2 matrices\nM1 = [randn(2, 2) for _ in 1:n_matrix_samples]\nM2 = [randn(2, 2) for _ in 1:n_matrix_samples]\nscalars = randn(n_matrix_samples)\n\n# True relationship: Result = scalar * M1 * M2\ntrue_matrices = [scalars[i] * M1[i] * M2[i] for i in 1:n_matrix_samples]\n\nprintln(\"  Generated $n_matrix_samples 2x2 matrices\")\nprintln(\"  Relationship: R = s * M1 * M2\")\nprintln(\"  This demonstrates matrix multiplication capabilities\")\n\n# Example with cross products\nprintln()\nprintln(\"Cross product example:\")\nv1_cross = [randn(3) for _ in 1:100]\nv2_cross = [randn(3) for _ in 1:100]\ncross_results = [cross(v1_cross[i], v2_cross[i]) for i in 1:100]\n\nprintln(\"  Generated 100 3D vector pairs\")\nprintln(\"  Relationship: c = v1 × v2 (cross product)\")\nprintln(\"  This demonstrates vector cross product capabilities\")","category":"page"},{"location":"examples/tensor-regression.html#Acceleration","page":"Tensor Regression","title":"Acceleration","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"[ ] Under developement!","category":"page"},{"location":"examples/tensor-regression.html#Best-Practices-Summary","page":"Tensor Regression","title":"Best Practices Summary","text":"","category":"section"},{"location":"examples/tensor-regression.html#1.-Problem-Formulation","page":"Tensor Regression","title":"1. Problem Formulation","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Start with simple tensor operations before moving to complex ones\nEnsure tensor dimensions are consistent throughout the problem\nConsider the physical or mathematical meaning of tensor operations","category":"page"},{"location":"examples/tensor-regression.html#2.-Data-Preparation","page":"Tensor Regression","title":"2. Data Preparation","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Normalize tensor components to similar scales\nHandle tensor symmetries appropriately (e.g., stress/strain tensors)\nValidate tensor data for physical plausibility","category":"page"},{"location":"examples/tensor-regression.html#3.-Algorithm-Configuration","page":"Tensor Regression","title":"3. Algorithm Configuration","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Use smaller populations initially due to computational cost\nReduce gene count and head length for faster convergence\nMonitor memory usage and use batch processing if needed","category":"page"},{"location":"examples/tensor-regression.html#4.-Performance-Optimization","page":"Tensor Regression","title":"4. Performance Optimization","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Consider GPU acceleration for large problems\nUse parallel processing when available\nImplement efficient tensor operations","category":"page"},{"location":"examples/tensor-regression.html#5.-Result-Validation","page":"Tensor Regression","title":"5. Result Validation","text":"","category":"section"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Validate results on independent test data\nCheck component-wise accuracy\nVerify tensor properties (symmetry, positive definiteness, etc.)\nCompare with known analytical solutions when available","category":"page"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"Tensor regression in GeneExpressionProgramming.jl opens up new possibilities for discovering complex mathematical relationships involving higher-dimensional data structures. While computationally more demanding than scalar regression, it provides unique capabilities for applications in physics, engineering, and computer science where tensor operations are fundamental.","category":"page"},{"location":"examples/tensor-regression.html","page":"Tensor Regression","title":"Tensor Regression","text":"","category":"page"}]
}
